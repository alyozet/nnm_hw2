{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNM_hw2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Домашнее задание 2\n",
        "\n",
        "пока можете прокручивать."
      ],
      "metadata": {
        "id": "JuH-K861GZuV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HvzpiYWyQNZ",
        "outputId": "e4f6af9b-e8f7-4855-fcdf-5a22605a4a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 791 kB 11.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 374 kB 50.7 MB/s \n",
            "\u001b[?25h  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.24 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.30.1 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q ipdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4If0De0n9OpP",
        "outputId": "1b897432-2d3e-431e-e22e-0ed484d63545"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 27.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 92 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 163 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 174 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 184 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 194 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 235 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 245 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 256 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 266 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 276 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 286 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 296 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 317 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 327 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 332 kB 11.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "import ipdb"
      ],
      "metadata": {
        "id": "yGvAztG7yTx-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "import ipdb"
      ],
      "metadata": {
        "id": "Vu3fQ2gt9HKe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "53Qiz0PwgVlY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начинаем: скачиваем файлы, делаем датафреймы, делим на тест и трейн, препроцессим, создаем на будущее словари.."
      ],
      "metadata": {
        "id": "sgaZYnrXGkpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tweets = pd.read_csv('positive.csv', encoding='utf-8', sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n",
        "neg_tweets = pd.read_csv('negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\n",
        "neg_tweets['tone'] = 0\n",
        "all_tweets_data = pos_tweets.append(neg_tweets)\n",
        "print(len(all_tweets_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bso0a8kNyjQD",
        "outputId": "5bde55a4-d767-461b-baff-72723150965a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_data = shuffle(all_tweets_data[['text','tone']])[:100000]"
      ],
      "metadata": {
        "id": "TJS-lBPfzmXL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, val_sentences = train_test_split(tweets_data, test_size=0.1)"
      ],
      "metadata": {
        "id": "Ca5PKd28z5uW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "02S3ugmvz8Gx",
        "outputId": "87969108-8064-4557-d4c5-6cc0c8dff6d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>82217</th>\n",
              "      <td>@De_nativo Будем считать что красные яблочки и...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74458</th>\n",
              "      <td>В том, что это может быть провокация, я не в о...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34731</th>\n",
              "      <td>RT @silver_xenia: температура( совсем плохо</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27934</th>\n",
              "      <td>@sassy_lolz да ведь не за что)\\nэто просто мне...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19779</th>\n",
              "      <td>@vesti_kpss из некогда монстров разве что Уотр...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49064</th>\n",
              "      <td>RT @VmesteCitations: Ну праааавда(( http://t.c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36093</th>\n",
              "      <td>\"только похоже я одна его буду пить, потому чт...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72278</th>\n",
              "      <td>Ахаха\\nСаша блин.\\nКак всегда.\\nПросто иди нах...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4151</th>\n",
              "      <td>RT @pedazyzuvab: Да... Давно сердце не болело....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77970</th>\n",
              "      <td>@sport_by если команда так решила,значит им ви...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  tone\n",
              "82217  @De_nativo Будем считать что красные яблочки и...     1\n",
              "74458  В том, что это может быть провокация, я не в о...     1\n",
              "34731        RT @silver_xenia: температура( совсем плохо     0\n",
              "27934  @sassy_lolz да ведь не за что)\\nэто просто мне...     1\n",
              "19779  @vesti_kpss из некогда монстров разве что Уотр...     0\n",
              "49064  RT @VmesteCitations: Ну праааавда(( http://t.c...     0\n",
              "36093  \"только похоже я одна его буду пить, потому чт...     1\n",
              "72278  Ахаха\\nСаша блин.\\nКак всегда.\\nПросто иди нах...     1\n",
              "4151   RT @pedazyzuvab: Да... Давно сердце не болело....     0\n",
              "77970  @sport_by если команда так решила,значит им ви...     1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [token.strip(punctuation) for token in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "OMYAGY8c0ZoR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter()\n",
        "\n",
        "for text in tweets_data['text']:\n",
        "    vocab.update(preprocess(text))\n",
        "print('всего уникальных токенов:', len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBJdGKIo0cqC",
        "outputId": "9c1484e1-ce2e-48c6-ff15-428fa144f64d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 202880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 2:\n",
        "        filtered_vocab.add(word)\n",
        "print('уникальных токенов, втретившихся больше 2 раз:', len(filtered_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ebgrN1n0eiD",
        "outputId": "e5ea4dcf-a9a1-4527-be2e-beb3a7cfabe6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных токенов, втретившихся больше 2 раз: 32444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)"
      ],
      "metadata": {
        "id": "h_kVQ_W40hAG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начинаем притрагиваться к нейросетям."
      ],
      "metadata": {
        "id": "zClBOf7pG8D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2word = {i:word for word, i in word2id.items()}"
      ],
      "metadata": {
        "id": "Bg7ns6pI0m1c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XMCwBrm0njJ",
        "outputId": "1369cf15-17c1-4e5c-a03b-e6771550be8b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): \n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        tokens = self.preprocess(self.dataset[index]) \n",
        "        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        tokens = text.lower().split()\n",
        "        tokens = [token.strip(punctuation) for token in tokens]\n",
        "        tokens = [token for token in tokens if token]\n",
        "        return tokens\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_ids, y"
      ],
      "metadata": {
        "id": "ESsuf5ga0_eR"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TweetsDataset(train_sentences, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "metadata": {
        "id": "TNVNNBbg1NDk"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = TweetsDataset(val_sentences, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "metadata": {
        "id": "jYM31nN01WC8"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN0(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.svertka = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        embedded = self.embedding(word) # эмбеддинги\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.bigrams(embedded) # свертка первая\n",
        "        feature_map_trigrams = self.trigrams(embedded) # свертка вторая\n",
        "        concat1 = torch.cat((feature_map_bigrams, feature_map_trigrams), 1) # конкатенируем\n",
        "        sv = self.pooling(self.svertka(concat1))\n",
        "        pooling0 = sv.max(2)[0] # макспулинг\n",
        "        \n",
        "        logits = self.hidden(pooling0) #линейный\n",
        "        logits = self.out(logits)  # сигмоида    \n",
        "        return logits"
      ],
      "metadata": {
        "id": "0eb_KvRyuRuZ"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 \n",
        "\n",
        "    model.train()  \n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): \n",
        "        optimizer.zero_grad() \n",
        "        preds = model(texts)  \n",
        "        loss = criterion(preds, ys)   \n",
        "        loss.backward()   \n",
        "        optimizer.step() \n",
        "        epoch_loss += loss.item() \n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) "
      ],
      "metadata": {
        "id": "Ny42uIw58h7j"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  \n",
        "            loss = criterion(preds, ys)  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/(i+1)}, Val f1: {epoch_metric/(i+1)}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) "
      ],
      "metadata": {
        "id": "Ipm2WLQ88lLz"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model0 = CNN0(len(word2id), 100)\n",
        "optimizer = optim.Adam(model0.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model0 = model0.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "metadata": {
        "id": "YgaSlB7uucsd"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model0, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model0, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model0, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB8F5Wy-utLt",
        "outputId": "2907c8dd-4fea-4244-f708-2d819dedc26f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7146475538611412\n",
            "Train loss: 0.6799866448749196\n",
            "Train loss: 0.663764420747757\n",
            "Train loss: 0.6528879154973956\n",
            "Train loss: 0.645861240369933\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5946033000946045, Val f1: 0.6676478385925293\n",
            "Val loss: 0.595921961700215, Val f1: 0.6654272079467773\n",
            "Val loss: 0.5962702690386305, Val f1: 0.6665107607841492\n",
            "Val loss: 0.5959943795905394, Val f1: 0.6674714684486389\n",
            "Val loss: 0.596836397227119, Val f1: 0.6658844351768494\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6090579032897949, Val f1: 0.645500898361206\n",
            "Val loss: 0.6073446124792099, Val f1: 0.6486740112304688\n",
            "Val loss: 0.6082810461521149, Val f1: 0.6483373641967773\n",
            "Val loss: 0.6083066537976265, Val f1: 0.6477082967758179\n",
            "Val loss: 0.6088057637214661, Val f1: 0.6511055827140808\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6325365677475929\n",
            "Train loss: 0.6108519471052921\n",
            "Train loss: 0.6020670986175537\n",
            "Train loss: 0.5968398398427821\n",
            "Train loss: 0.5932337471417019\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5525272769086501, Val f1: 0.7221663594245911\n",
            "Val loss: 0.5514400916941026, Val f1: 0.724207878112793\n",
            "Val loss: 0.5527607599894205, Val f1: 0.7230679392814636\n",
            "Val loss: 0.5535342228763244, Val f1: 0.7214952707290649\n",
            "Val loss: 0.5530302952317631, Val f1: 0.7218640446662903\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5847654640674591, Val f1: 0.6808716058731079\n",
            "Val loss: 0.5818209052085876, Val f1: 0.6858897805213928\n",
            "Val loss: 0.5817465285460154, Val f1: 0.6805243492126465\n",
            "Val loss: 0.5829206928610802, Val f1: 0.6782775521278381\n",
            "Val loss: 0.5831689357757568, Val f1: 0.6825894117355347\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.5869550742208958\n",
            "Train loss: 0.565968520713575\n",
            "Train loss: 0.5583013451099396\n",
            "Train loss: 0.555829052604846\n",
            "Train loss: 0.552958546649842\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5159831187304329, Val f1: 0.7544875741004944\n",
            "Val loss: 0.5133438005166895, Val f1: 0.7548568248748779\n",
            "Val loss: 0.5129914301283219, Val f1: 0.7564271688461304\n",
            "Val loss: 0.5122740974321085, Val f1: 0.7581831812858582\n",
            "Val loss: 0.5126644807703354, Val f1: 0.757878839969635\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5696487128734589, Val f1: 0.6923531293869019\n",
            "Val loss: 0.5651665180921555, Val f1: 0.6986316442489624\n",
            "Val loss: 0.5647997558116913, Val f1: 0.6976382732391357\n",
            "Val loss: 0.5668591558933258, Val f1: 0.6951267719268799\n",
            "Val loss: 0.5667487442493438, Val f1: 0.6996831297874451\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.5461583714932203\n",
            "Train loss: 0.5273133967861985\n",
            "Train loss: 0.5202417546510696\n",
            "Train loss: 0.5153433035558729\n",
            "Train loss: 0.5145857905348142\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4816733914263108, Val f1: 0.7877795696258545\n",
            "Val loss: 0.4783477607895346, Val f1: 0.793422281742096\n",
            "Val loss: 0.4773287773132324, Val f1: 0.7952761650085449\n",
            "Val loss: 0.4779432087260134, Val f1: 0.794783353805542\n",
            "Val loss: 0.4780038910753587, Val f1: 0.7949550747871399\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5623612105846405, Val f1: 0.7240554094314575\n",
            "Val loss: 0.5591804832220078, Val f1: 0.726996898651123\n",
            "Val loss: 0.559686928987503, Val f1: 0.7213858962059021\n",
            "Val loss: 0.5621780827641487, Val f1: 0.7191027998924255\n",
            "Val loss: 0.5613980531692505, Val f1: 0.7223030924797058\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.4985580574721098\n",
            "Train loss: 0.48442007917346375\n",
            "Train loss: 0.4800966089963913\n",
            "Train loss: 0.47732539630647913\n",
            "Train loss: 0.47726639040878843\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4268480030929341, Val f1: 0.8128279447555542\n",
            "Val loss: 0.4295266743968515, Val f1: 0.8120254278182983\n",
            "Val loss: 0.43079648181503893, Val f1: 0.8109875917434692\n",
            "Val loss: 0.4314031118855757, Val f1: 0.8108595013618469\n",
            "Val loss: 0.4319675080916461, Val f1: 0.8115401268005371\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5448855757713318, Val f1: 0.715603232383728\n",
            "Val loss: 0.5418744832277298, Val f1: 0.7173898220062256\n",
            "Val loss: 0.5434527198473612, Val f1: 0.7138389348983765\n",
            "Val loss: 0.5481147393584251, Val f1: 0.7095637321472168\n",
            "Val loss: 0.5480912983417511, Val f1: 0.7122913599014282\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.45706474781036377\n",
            "Train loss: 0.4465441523176251\n",
            "Train loss: 0.442280136346817\n",
            "Train loss: 0.44072817733038716\n",
            "Train loss: 0.4402694230278333\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.39212270168697133, Val f1: 0.8340278267860413\n",
            "Val loss: 0.39399126084411845, Val f1: 0.8313688039779663\n",
            "Val loss: 0.3960131231476279, Val f1: 0.829388439655304\n",
            "Val loss: 0.39770054598064986, Val f1: 0.8273674845695496\n",
            "Val loss: 0.39762272238731383, Val f1: 0.8273982405662537\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5445257723331451, Val f1: 0.7136507034301758\n",
            "Val loss: 0.5421673059463501, Val f1: 0.7123258113861084\n",
            "Val loss: 0.5448100169499716, Val f1: 0.7057477831840515\n",
            "Val loss: 0.5501531660556793, Val f1: 0.702855110168457\n",
            "Val loss: 0.5512774288654327, Val f1: 0.7046256065368652\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.41912529431283474\n",
            "Train loss: 0.4092951543403394\n",
            "Train loss: 0.40504463136196134\n",
            "Train loss: 0.4033358261656405\n",
            "Train loss: 0.4034458114987328\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.35692330318338733, Val f1: 0.8629164099693298\n",
            "Val loss: 0.35944895446300507, Val f1: 0.8600723147392273\n",
            "Val loss: 0.3613131636498021, Val f1: 0.8597986698150635\n",
            "Val loss: 0.36127450623933005, Val f1: 0.8589917421340942\n",
            "Val loss: 0.3613185283015756, Val f1: 0.858970046043396\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5572319328784943, Val f1: 0.7277289628982544\n",
            "Val loss: 0.5580170452594757, Val f1: 0.7318238615989685\n",
            "Val loss: 0.5556872487068176, Val f1: 0.7299412488937378\n",
            "Val loss: 0.5607059523463249, Val f1: 0.7269816994667053\n",
            "Val loss: 0.5598369002342224, Val f1: 0.7290403246879578\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.3838757574558258\n",
            "Train loss: 0.37250401666670135\n",
            "Train loss: 0.3704295271635056\n",
            "Train loss: 0.36865626609147484\n",
            "Train loss: 0.3678819444917497\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3232482899637783, Val f1: 0.8775351643562317\n",
            "Val loss: 0.32147133175064535, Val f1: 0.877843976020813\n",
            "Val loss: 0.3206627222837186, Val f1: 0.8773654699325562\n",
            "Val loss: 0.3219459306667833, Val f1: 0.8769329786300659\n",
            "Val loss: 0.32120675479664523, Val f1: 0.8772315979003906\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5565357208251953, Val f1: 0.7106199264526367\n",
            "Val loss: 0.5559398382902145, Val f1: 0.7162029147148132\n",
            "Val loss: 0.5577746033668518, Val f1: 0.7133259177207947\n",
            "Val loss: 0.5632888749241829, Val f1: 0.7114084959030151\n",
            "Val loss: 0.5641269207000732, Val f1: 0.7116472721099854\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.3388756085187197\n",
            "Train loss: 0.333048321984031\n",
            "Train loss: 0.3309313452243805\n",
            "Train loss: 0.3318932563511293\n",
            "Train loss: 0.33170372389611746\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.28840557967915254, Val f1: 0.8992452025413513\n",
            "Val loss: 0.28606366672936606, Val f1: 0.9010804295539856\n",
            "Val loss: 0.2851834384834065, Val f1: 0.9016971588134766\n",
            "Val loss: 0.28559330850839615, Val f1: 0.9008380770683289\n",
            "Val loss: 0.2860704730538761, Val f1: 0.900691032409668\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.564576655626297, Val f1: 0.7171415090560913\n",
            "Val loss: 0.564617931842804, Val f1: 0.7241339683532715\n",
            "Val loss: 0.5670163830121359, Val f1: 0.7214159369468689\n",
            "Val loss: 0.5726621001958847, Val f1: 0.719887912273407\n",
            "Val loss: 0.5724682867527008, Val f1: 0.7197409868240356\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.30219376273453236\n",
            "Train loss: 0.2959981154311787\n",
            "Train loss: 0.2966821086406708\n",
            "Train loss: 0.2976246405003676\n",
            "Train loss: 0.29860614453043255\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2538822933154948, Val f1: 0.9144803881645203\n",
            "Val loss: 0.25147125913816337, Val f1: 0.9174394607543945\n",
            "Val loss: 0.2518880110160977, Val f1: 0.9171382188796997\n",
            "Val loss: 0.25136516769142714, Val f1: 0.9173219799995422\n",
            "Val loss: 0.2517943000092226, Val f1: 0.917422890663147\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5803734660148621, Val f1: 0.7215807437896729\n",
            "Val loss: 0.5817194432020187, Val f1: 0.7291545867919922\n",
            "Val loss: 0.5826675494511923, Val f1: 0.7258509993553162\n",
            "Val loss: 0.587414339184761, Val f1: 0.7233738303184509\n",
            "Val loss: 0.5877662122249603, Val f1: 0.7240191102027893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hz6nNs8Lwc0t",
        "outputId": "b2393712-b48b-4503-f2f0-c5ca9b3a4188"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выглядит отстойно: во-перых очевидное переобучение, во-вторых в лучшем случае лосс на тесте около 0,55, это как-то не оч.\n",
        "\n",
        "Улучшим дропаутами, пулингами и релу между сверточными слоями:"
      ],
      "metadata": {
        "id": "cSANmJmizxAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.svertka = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.dropout(self.pooling(self.relu(self.bigrams(embedded))))\n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams(embedded))))\n",
        "        concat1 = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        sv = self.pooling(self.relu(self.svertka(concat1)))\n",
        "        pooling0 = sv.max(2)[0]\n",
        "        \n",
        "        logits = self.hidden(pooling0) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "metadata": {
        "id": "UUQ970OY1xZ1"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN(len(word2id), 100)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "metadata": {
        "id": "w_7HYU0167Nq"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW3HxBOFf75j",
        "outputId": "c308a166-bb77-4e19-8d21-79587ea23b17"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7397546470165253\n",
            "Train loss: 0.7109098705378446\n",
            "Train loss: 0.6986915159225464\n",
            "Train loss: 0.6897014566321871\n",
            "Train loss: 0.6820289662906102\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6373868935248431, Val f1: 0.6449377536773682\n",
            "Val loss: 0.6353673005805296, Val f1: 0.6482030153274536\n",
            "Val loss: 0.634827061026704, Val f1: 0.6487230062484741\n",
            "Val loss: 0.6346185356378555, Val f1: 0.6483426094055176\n",
            "Val loss: 0.6340846293112811, Val f1: 0.6495673656463623\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6380287110805511, Val f1: 0.6416146755218506\n",
            "Val loss: 0.6369955092668533, Val f1: 0.6448604464530945\n",
            "Val loss: 0.6379000047842661, Val f1: 0.6395613551139832\n",
            "Val loss: 0.6375073716044426, Val f1: 0.640622615814209\n",
            "Val loss: 0.6380674004554748, Val f1: 0.6427370309829712\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6782318986952305\n",
            "Train loss: 0.6550261125420079\n",
            "Train loss: 0.6467426145076751\n",
            "Train loss: 0.6400342183326607\n",
            "Train loss: 0.6364160925149918\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6046747530207914, Val f1: 0.6808321475982666\n",
            "Val loss: 0.6017463224775651, Val f1: 0.6822835206985474\n",
            "Val loss: 0.6020915181029076, Val f1: 0.6830916404724121\n",
            "Val loss: 0.602445628713159, Val f1: 0.6817286014556885\n",
            "Val loss: 0.6022256823147044, Val f1: 0.6822673678398132\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6107679307460785, Val f1: 0.6656491756439209\n",
            "Val loss: 0.6091085821390152, Val f1: 0.6681734323501587\n",
            "Val loss: 0.6100229918956757, Val f1: 0.6649535298347473\n",
            "Val loss: 0.6107271537184715, Val f1: 0.6648236513137817\n",
            "Val loss: 0.6113685369491577, Val f1: 0.6682981848716736\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6445892862975597\n",
            "Train loss: 0.624200884139899\n",
            "Train loss: 0.6168022859096527\n",
            "Train loss: 0.6120661586078245\n",
            "Train loss: 0.6096445385898862\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5742384090143091, Val f1: 0.6890832185745239\n",
            "Val loss: 0.5732126078184914, Val f1: 0.6923937201499939\n",
            "Val loss: 0.5742535976802602, Val f1: 0.6919922232627869\n",
            "Val loss: 0.5749932571369059, Val f1: 0.6913927793502808\n",
            "Val loss: 0.5754014407887178, Val f1: 0.6913976669311523\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5907192826271057, Val f1: 0.6742156744003296\n",
            "Val loss: 0.587789312005043, Val f1: 0.6717002391815186\n",
            "Val loss: 0.5882931848367056, Val f1: 0.6703631281852722\n",
            "Val loss: 0.5900489464402199, Val f1: 0.6676733493804932\n",
            "Val loss: 0.5918201029300689, Val f1: 0.6690341830253601\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.62372962012887\n",
            "Train loss: 0.6035075187683105\n",
            "Train loss: 0.5933433616161347\n",
            "Train loss: 0.5884715976999767\n",
            "Train loss: 0.5862229608354115\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5586128024493947, Val f1: 0.671428382396698\n",
            "Val loss: 0.5581697558655458, Val f1: 0.6701265573501587\n",
            "Val loss: 0.5577647627568713, Val f1: 0.6701486110687256\n",
            "Val loss: 0.5582550264456693, Val f1: 0.6696020364761353\n",
            "Val loss: 0.5582287374664755, Val f1: 0.670314610004425\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.579934149980545, Val f1: 0.644066572189331\n",
            "Val loss: 0.5764384269714355, Val f1: 0.6380990147590637\n",
            "Val loss: 0.5765979488690695, Val f1: 0.6312548518180847\n",
            "Val loss: 0.5797496661543846, Val f1: 0.6276266574859619\n",
            "Val loss: 0.5828284919261932, Val f1: 0.6293169260025024\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.6002104468643665\n",
            "Train loss: 0.5807821786764896\n",
            "Train loss: 0.570614869594574\n",
            "Train loss: 0.5669344334460017\n",
            "Train loss: 0.5656803590910775\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5365314834258136, Val f1: 0.7469927668571472\n",
            "Val loss: 0.5345366772483376, Val f1: 0.747905433177948\n",
            "Val loss: 0.5327481323597478, Val f1: 0.749467670917511\n",
            "Val loss: 0.5318694570485283, Val f1: 0.7495236992835999\n",
            "Val loss: 0.5322602180873647, Val f1: 0.7498411536216736\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.562897652387619, Val f1: 0.7182179093360901\n",
            "Val loss: 0.5605570673942566, Val f1: 0.7163417935371399\n",
            "Val loss: 0.5596939225991567, Val f1: 0.715381383895874\n",
            "Val loss: 0.5623935610055923, Val f1: 0.7150173783302307\n",
            "Val loss: 0.5644486665725708, Val f1: 0.7161899209022522\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5751610137522221\n",
            "Train loss: 0.5579946871959802\n",
            "Train loss: 0.5507625150680542\n",
            "Train loss: 0.5468105584827821\n",
            "Train loss: 0.545015961641357\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5129314117571887, Val f1: 0.7656430602073669\n",
            "Val loss: 0.5134646515635883, Val f1: 0.7669541835784912\n",
            "Val loss: 0.5106560471011143, Val f1: 0.7681195139884949\n",
            "Val loss: 0.5114754355129074, Val f1: 0.7673912644386292\n",
            "Val loss: 0.5116712749004364, Val f1: 0.7671886086463928\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5544155538082123, Val f1: 0.7272753715515137\n",
            "Val loss: 0.5521495193243027, Val f1: 0.7243425250053406\n",
            "Val loss: 0.5502757926781973, Val f1: 0.7245585322380066\n",
            "Val loss: 0.5532648116350174, Val f1: 0.7222131490707397\n",
            "Val loss: 0.555845296382904, Val f1: 0.7236656546592712\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5494008846580982\n",
            "Train loss: 0.5353409232515277\n",
            "Train loss: 0.5303651314973831\n",
            "Train loss: 0.5275762961871588\n",
            "Train loss: 0.5261957212573006\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.49476002069080577, Val f1: 0.7802313566207886\n",
            "Val loss: 0.4937807891298743, Val f1: 0.7793901562690735\n",
            "Val loss: 0.49412282892301973, Val f1: 0.7784906029701233\n",
            "Val loss: 0.49319685119039874, Val f1: 0.7789037823677063\n",
            "Val loss: 0.4932956593878129, Val f1: 0.7796604037284851\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5473135113716125, Val f1: 0.7336162328720093\n",
            "Val loss: 0.5438058525323868, Val f1: 0.7325866222381592\n",
            "Val loss: 0.5411780774593353, Val f1: 0.7314948439598083\n",
            "Val loss: 0.5443502739071846, Val f1: 0.7297043800354004\n",
            "Val loss: 0.547387671470642, Val f1: 0.729594886302948\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.533998180180788\n",
            "Train loss: 0.5177445411682129\n",
            "Train loss: 0.5131630623340606\n",
            "Train loss: 0.5111777248667247\n",
            "Train loss: 0.5084474083213579\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.46105091361438527, Val f1: 0.7923524975776672\n",
            "Val loss: 0.4649057142874774, Val f1: 0.7879543900489807\n",
            "Val loss: 0.46683812667341795, Val f1: 0.7862239480018616\n",
            "Val loss: 0.467495124129688, Val f1: 0.7857641577720642\n",
            "Val loss: 0.4679330769707175, Val f1: 0.7856348752975464\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5333713889122009, Val f1: 0.7269684076309204\n",
            "Val loss: 0.5292032957077026, Val f1: 0.7297313809394836\n",
            "Val loss: 0.5283359785874685, Val f1: 0.7282330393791199\n",
            "Val loss: 0.5334567949175835, Val f1: 0.7247621417045593\n",
            "Val loss: 0.5371544420719147, Val f1: 0.7252113819122314\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5190333612263203\n",
            "Train loss: 0.5028054696140867\n",
            "Train loss: 0.4982071566581726\n",
            "Train loss: 0.495195505779181\n",
            "Train loss: 0.4939298871017638\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4546438736074111, Val f1: 0.7965748906135559\n",
            "Val loss: 0.4547203884405248, Val f1: 0.7950730919837952\n",
            "Val loss: 0.4544856898924884, Val f1: 0.7954702377319336\n",
            "Val loss: 0.45401468084138985, Val f1: 0.7965165376663208\n",
            "Val loss: 0.4533058226108551, Val f1: 0.7966049313545227\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5271618664264679, Val f1: 0.7331847548484802\n",
            "Val loss: 0.523315355181694, Val f1: 0.7332095503807068\n",
            "Val loss: 0.5221534272034963, Val f1: 0.7309402823448181\n",
            "Val loss: 0.5272926241159439, Val f1: 0.7280339002609253\n",
            "Val loss: 0.5310758173465728, Val f1: 0.7286858558654785\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.505639074370265\n",
            "Train loss: 0.48648475245995954\n",
            "Train loss: 0.4835282409191132\n",
            "Train loss: 0.4787803487991219\n",
            "Train loss: 0.47683566347474143\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.43401679571937113, Val f1: 0.8091003894805908\n",
            "Val loss: 0.43516217347453623, Val f1: 0.8126190304756165\n",
            "Val loss: 0.43770330676845476, Val f1: 0.809747040271759\n",
            "Val loss: 0.43673577291124005, Val f1: 0.8100118041038513\n",
            "Val loss: 0.4366834510775173, Val f1: 0.8093577027320862\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5249791145324707, Val f1: 0.7352092266082764\n",
            "Val loss: 0.5196506083011627, Val f1: 0.7402710914611816\n",
            "Val loss: 0.518985370794932, Val f1: 0.7393440008163452\n",
            "Val loss: 0.5247942805290222, Val f1: 0.736342191696167\n",
            "Val loss: 0.5290733277797699, Val f1: 0.7360274791717529\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.4814403187483549\n",
            "Train loss: 0.4699336127801375\n",
            "Train loss: 0.4666601848602295\n",
            "Train loss: 0.46429021812196986\n",
            "Train loss: 0.4621927723998115\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4168335900587194, Val f1: 0.8182922601699829\n",
            "Val loss: 0.4166248020003824, Val f1: 0.818397581577301\n",
            "Val loss: 0.4185355650443657, Val f1: 0.8173218965530396\n",
            "Val loss: 0.4198179525487563, Val f1: 0.8157423138618469\n",
            "Val loss: 0.42070210681242104, Val f1: 0.8152185082435608\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5206282734870911, Val f1: 0.7437377572059631\n",
            "Val loss: 0.5153379291296005, Val f1: 0.7434912323951721\n",
            "Val loss: 0.5147342383861542, Val f1: 0.7396600246429443\n",
            "Val loss: 0.521421030163765, Val f1: 0.7372339963912964\n",
            "Val loss: 0.5261080145835877, Val f1: 0.7364539504051208\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.4717826209962368\n",
            "Train loss: 0.45575186700531933\n",
            "Train loss: 0.45018039107322694\n",
            "Train loss: 0.45031199037139097\n",
            "Train loss: 0.4485630634285155\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.41182926472495585, Val f1: 0.8221026659011841\n",
            "Val loss: 0.40849637459306154, Val f1: 0.8248873949050903\n",
            "Val loss: 0.40772806487831414, Val f1: 0.8247341513633728\n",
            "Val loss: 0.40788189365583305, Val f1: 0.8247206211090088\n",
            "Val loss: 0.40668224797529334, Val f1: 0.82587730884552\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5204907953739166, Val f1: 0.7462747097015381\n",
            "Val loss: 0.5142201334238052, Val f1: 0.7454696893692017\n",
            "Val loss: 0.5126486271619797, Val f1: 0.741352915763855\n",
            "Val loss: 0.5192825756967068, Val f1: 0.7396043539047241\n",
            "Val loss: 0.5242731660604477, Val f1: 0.7405891418457031\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.45210875384509563\n",
            "Train loss: 0.43965189745931915\n",
            "Train loss: 0.43397339403629304\n",
            "Train loss: 0.43330212893770703\n",
            "Train loss: 0.4352165095153309\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3961203711874345, Val f1: 0.8347219228744507\n",
            "Val loss: 0.39876438852618723, Val f1: 0.8314213752746582\n",
            "Val loss: 0.4003608250150494, Val f1: 0.8307862877845764\n",
            "Val loss: 0.3987976172391106, Val f1: 0.8326917290687561\n",
            "Val loss: 0.39806813141878916, Val f1: 0.8330413699150085\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5193652212619781, Val f1: 0.7475159168243408\n",
            "Val loss: 0.5131060183048248, Val f1: 0.7491785287857056\n",
            "Val loss: 0.5116287569204966, Val f1: 0.746650218963623\n",
            "Val loss: 0.518544115126133, Val f1: 0.7440587878227234\n",
            "Val loss: 0.5232064902782441, Val f1: 0.7444884777069092\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.4332236386835575\n",
            "Train loss: 0.4243555556644093\n",
            "Train loss: 0.4219642460346222\n",
            "Train loss: 0.42060882386876575\n",
            "Train loss: 0.42223506243455977\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3855430694187389, Val f1: 0.8378322720527649\n",
            "Val loss: 0.38262270653949065, Val f1: 0.8384747505187988\n",
            "Val loss: 0.3827142569364286, Val f1: 0.8384982347488403\n",
            "Val loss: 0.3823363461038646, Val f1: 0.8386651277542114\n",
            "Val loss: 0.3823638972114114, Val f1: 0.8386645913124084\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5198431313037872, Val f1: 0.7352406978607178\n",
            "Val loss: 0.5123156905174255, Val f1: 0.7431167364120483\n",
            "Val loss: 0.5104161898295084, Val f1: 0.7424635291099548\n",
            "Val loss: 0.5178084895014763, Val f1: 0.7411311864852905\n",
            "Val loss: 0.5227952122688293, Val f1: 0.7418997287750244\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.43194410391151905\n",
            "Train loss: 0.4193782562559301\n",
            "Train loss: 0.4143638467788696\n",
            "Train loss: 0.4117639456222306\n",
            "Train loss: 0.4118934231145041\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3705715642255895, Val f1: 0.8460726737976074\n",
            "Val loss: 0.3707193329053767, Val f1: 0.8483531475067139\n",
            "Val loss: 0.3690337912709105, Val f1: 0.8492103815078735\n",
            "Val loss: 0.36957165774177103, Val f1: 0.8484728932380676\n",
            "Val loss: 0.36910772113239065, Val f1: 0.8488802909851074\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5251857936382294, Val f1: 0.7495629787445068\n",
            "Val loss: 0.5174488723278046, Val f1: 0.7532080411911011\n",
            "Val loss: 0.5153247614701589, Val f1: 0.7511340379714966\n",
            "Val loss: 0.5232933908700943, Val f1: 0.7491921186447144\n",
            "Val loss: 0.5288066923618316, Val f1: 0.7492581009864807\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.41503703966736794\n",
            "Train loss: 0.40583855094331683\n",
            "Train loss: 0.4001148051023483\n",
            "Train loss: 0.3984193432686934\n",
            "Train loss: 0.39872505444855916\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.35922762225655946, Val f1: 0.8521073460578918\n",
            "Val loss: 0.3601927441709182, Val f1: 0.8500746488571167\n",
            "Val loss: 0.35778190224778417, Val f1: 0.8531106114387512\n",
            "Val loss: 0.357409846256761, Val f1: 0.8540317416191101\n",
            "Val loss: 0.35773990750312806, Val f1: 0.8539212346076965\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5251359343528748, Val f1: 0.7533276677131653\n",
            "Val loss: 0.5181664228439331, Val f1: 0.7558332681655884\n",
            "Val loss: 0.5164226293563843, Val f1: 0.7538754940032959\n",
            "Val loss: 0.5242434144020081, Val f1: 0.7519326210021973\n",
            "Val loss: 0.530157059431076, Val f1: 0.7511271834373474\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.3989548087120056\n",
            "Train loss: 0.39101017785794806\n",
            "Train loss: 0.38855550408363343\n",
            "Train loss: 0.38924979051547265\n",
            "Train loss: 0.3864534305674689\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3374113212613499, Val f1: 0.8660317063331604\n",
            "Val loss: 0.33732684570200305, Val f1: 0.862411618232727\n",
            "Val loss: 0.33894762630556147, Val f1: 0.8618268370628357\n",
            "Val loss: 0.3396729805013713, Val f1: 0.8604423403739929\n",
            "Val loss: 0.33908614060458014, Val f1: 0.8606716394424438\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5248567461967468, Val f1: 0.7436437606811523\n",
            "Val loss: 0.5187190622091293, Val f1: 0.7506453990936279\n",
            "Val loss: 0.5187506477038065, Val f1: 0.7468972206115723\n",
            "Val loss: 0.5274115726351738, Val f1: 0.7454989552497864\n",
            "Val loss: 0.5336468517780304, Val f1: 0.7446438670158386\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.381572637706995\n",
            "Train loss: 0.37517030491973413\n",
            "Train loss: 0.37361617922782897\n",
            "Train loss: 0.374878731236529\n",
            "Train loss: 0.37564137258699964\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.33210113118676576, Val f1: 0.8653150796890259\n",
            "Val loss: 0.3306069207542083, Val f1: 0.8659691214561462\n",
            "Val loss: 0.33219464035595164, Val f1: 0.8655005097389221\n",
            "Val loss: 0.330718218403704, Val f1: 0.8673816323280334\n",
            "Val loss: 0.3312564667533426, Val f1: 0.8672283291816711\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5320087671279907, Val f1: 0.7467530965805054\n",
            "Val loss: 0.5226888880133629, Val f1: 0.7519075870513916\n",
            "Val loss: 0.5209136058886846, Val f1: 0.7509114146232605\n",
            "Val loss: 0.5290067233145237, Val f1: 0.7503107786178589\n",
            "Val loss: 0.5355897337198258, Val f1: 0.7493500709533691\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.38394724391400814\n",
            "Train loss: 0.37458511916073883\n",
            "Train loss: 0.3700784546136856\n",
            "Train loss: 0.3674510793009801\n",
            "Train loss: 0.3668916764713469\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.32144103856647716, Val f1: 0.8735017776489258\n",
            "Val loss: 0.32011041220496683, Val f1: 0.8732184171676636\n",
            "Val loss: 0.32116420304074006, Val f1: 0.8713441491127014\n",
            "Val loss: 0.32025467560571785, Val f1: 0.872075617313385\n",
            "Val loss: 0.31978169083595276, Val f1: 0.872385561466217\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.536703497171402, Val f1: 0.7452717423439026\n",
            "Val loss: 0.5256639868021011, Val f1: 0.7504660487174988\n",
            "Val loss: 0.5243468284606934, Val f1: 0.7515031099319458\n",
            "Val loss: 0.5329665467143059, Val f1: 0.7507566809654236\n",
            "Val loss: 0.5401442348957062, Val f1: 0.7499023675918579\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.36018520779907703\n",
            "Train loss: 0.3545983126669219\n",
            "Train loss: 0.35387097477912904\n",
            "Train loss: 0.3540741329762473\n",
            "Train loss: 0.35373532417274656\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.30652157699360566, Val f1: 0.8779054284095764\n",
            "Val loss: 0.30762771823826957, Val f1: 0.8774640560150146\n",
            "Val loss: 0.3068716940926571, Val f1: 0.8783115744590759\n",
            "Val loss: 0.306362006594153, Val f1: 0.8786940574645996\n",
            "Val loss: 0.3052369906621821, Val f1: 0.8787569999694824\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5392750799655914, Val f1: 0.7409003376960754\n",
            "Val loss: 0.528683215379715, Val f1: 0.7464166879653931\n",
            "Val loss: 0.5281881292661031, Val f1: 0.7482633590698242\n",
            "Val loss: 0.5370937585830688, Val f1: 0.7462255358695984\n",
            "Val loss: 0.5441977143287658, Val f1: 0.7458346486091614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "PVPoohvABf3R",
        "outputId": "b1ae94cd-3d5c-46a6-e59a-50f91c11cab6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "вроде получше?))) мы спаслись от переобучения, но лучший лосс на тесте не сильно изменился.\n",
        "\n",
        "Теперь попробуем с предобученными эмбеддингами:\n"
      ],
      "metadata": {
        "id": "_TQGaXE1z6sO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "texts = all_tweets_data.text.apply(preprocess).tolist()\n",
        "w2v = gensim.models.Word2Vec(texts, size=100, window=5, min_count=1)"
      ],
      "metadata": {
        "id": "LC_T20wIFttL"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = np.zeros((len(word2id), 100))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = w2v.wv[word]    \n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      # oov словам сопоставляем случайный вектор\n",
        "      weights[i] = np.random.normal(0,0.1,100)"
      ],
      "metadata": {
        "id": "33-xwYco4HW5"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_emb(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.svertka = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.dropout(self.pooling(self.relu(self.bigrams(embedded))))\n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams(embedded))))\n",
        "        concat1 = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        sv = self.pooling(self.relu(self.svertka(concat1)))\n",
        "        pooling0 = sv.max(2)[0]\n",
        "        \n",
        "        logits = self.hidden(pooling0) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "metadata": {
        "id": "gZYN-Cok4xN_"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = CNN_emb(len(word2id), 100)\n",
        "optimizer = optim.Adam(model2.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model2 = model2.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "metadata": {
        "id": "NkiTc_lA8gqR"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model2, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model2, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model2, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPu6osXI8zY-",
        "outputId": "b21d1916-9e83-432d-99ba-adc5a2316649"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7348674312233925\n",
            "Train loss: 0.7067410151163737\n",
            "Train loss: 0.6935847520828247\n",
            "Train loss: 0.6841898658382359\n",
            "Train loss: 0.6764296967358816\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6266885329695309, Val f1: 0.6096464991569519\n",
            "Val loss: 0.6250040601281559, Val f1: 0.6147249937057495\n",
            "Val loss: 0.6248770297742358, Val f1: 0.6149740815162659\n",
            "Val loss: 0.6257970368160921, Val f1: 0.6135932803153992\n",
            "Val loss: 0.6259050123831805, Val f1: 0.6147226095199585\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6286381185054779, Val f1: 0.6194698810577393\n",
            "Val loss: 0.6272288858890533, Val f1: 0.616374135017395\n",
            "Val loss: 0.6296482384204865, Val f1: 0.6119556427001953\n",
            "Val loss: 0.6292624026536942, Val f1: 0.6098564863204956\n",
            "Val loss: 0.6299563586711884, Val f1: 0.6109765768051147\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6709494367241859\n",
            "Train loss: 0.6481855999339711\n",
            "Train loss: 0.6397007489204407\n",
            "Train loss: 0.6344443577439037\n",
            "Train loss: 0.6303083619901112\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5934387480511385, Val f1: 0.6721011400222778\n",
            "Val loss: 0.5925427079200745, Val f1: 0.672693133354187\n",
            "Val loss: 0.5927835866516712, Val f1: 0.6721510887145996\n",
            "Val loss: 0.594812289756887, Val f1: 0.6713355183601379\n",
            "Val loss: 0.5950511315289666, Val f1: 0.6696482300758362\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.601832777261734, Val f1: 0.6631384491920471\n",
            "Val loss: 0.6013181954622269, Val f1: 0.6600792407989502\n",
            "Val loss: 0.6030362447102865, Val f1: 0.6557480692863464\n",
            "Val loss: 0.6029608324170113, Val f1: 0.6534321308135986\n",
            "Val loss: 0.6044809818267822, Val f1: 0.6564871072769165\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.640962939709425\n",
            "Train loss: 0.6203809308283257\n",
            "Train loss: 0.6127207696437835\n",
            "Train loss: 0.6091783696146154\n",
            "Train loss: 0.6057439226479757\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5661595568937414, Val f1: 0.7008163332939148\n",
            "Val loss: 0.5684717157307793, Val f1: 0.7006047964096069\n",
            "Val loss: 0.5687250833885342, Val f1: 0.6997802257537842\n",
            "Val loss: 0.5694929326281828, Val f1: 0.6989853978157043\n",
            "Val loss: 0.5702193063848159, Val f1: 0.699036180973053\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5845993161201477, Val f1: 0.6766630411148071\n",
            "Val loss: 0.5830138921737671, Val f1: 0.673521876335144\n",
            "Val loss: 0.5840453108151754, Val f1: 0.6700357794761658\n",
            "Val loss: 0.5848819240927696, Val f1: 0.6693130731582642\n",
            "Val loss: 0.5865967392921447, Val f1: 0.6713981032371521\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6165989823639393\n",
            "Train loss: 0.597851093971368\n",
            "Train loss: 0.5904761791229248\n",
            "Train loss: 0.5863188451795436\n",
            "Train loss: 0.5836908831482842\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.54214687908397, Val f1: 0.7249252200126648\n",
            "Val loss: 0.5455598410438088, Val f1: 0.7222421169281006\n",
            "Val loss: 0.5474326037893108, Val f1: 0.7205587029457092\n",
            "Val loss: 0.5481493341572145, Val f1: 0.7201729416847229\n",
            "Val loss: 0.5484446939300088, Val f1: 0.7191863656044006\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5742640495300293, Val f1: 0.6895353198051453\n",
            "Val loss: 0.5694333463907242, Val f1: 0.6861327290534973\n",
            "Val loss: 0.5699544548988342, Val f1: 0.6857659220695496\n",
            "Val loss: 0.5707293599843979, Val f1: 0.6842744946479797\n",
            "Val loss: 0.5725160539150238, Val f1: 0.6877306699752808\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.5985029488801956\n",
            "Train loss: 0.5784283858357053\n",
            "Train loss: 0.5719864571094513\n",
            "Train loss: 0.5677606134272334\n",
            "Train loss: 0.5649789734965279\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5263495515374577, Val f1: 0.7404027581214905\n",
            "Val loss: 0.5277976639130536, Val f1: 0.7408648729324341\n",
            "Val loss: 0.5274421841490502, Val f1: 0.7421613931655884\n",
            "Val loss: 0.5271485526772106, Val f1: 0.7423861622810364\n",
            "Val loss: 0.5274985919980442, Val f1: 0.7419758439064026\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5629533529281616, Val f1: 0.7113729119300842\n",
            "Val loss: 0.5563182234764099, Val f1: 0.708484411239624\n",
            "Val loss: 0.5562334656715393, Val f1: 0.7067674398422241\n",
            "Val loss: 0.5580778419971466, Val f1: 0.7032489776611328\n",
            "Val loss: 0.5600885689258576, Val f1: 0.7045344710350037\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5700944736599922\n",
            "Train loss: 0.5571430632562349\n",
            "Train loss: 0.5498991441726685\n",
            "Train loss: 0.5470959268399139\n",
            "Train loss: 0.5439088805800393\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5036540855379665, Val f1: 0.748953104019165\n",
            "Val loss: 0.5035523263847127, Val f1: 0.7496918439865112\n",
            "Val loss: 0.5068084019071916, Val f1: 0.7466475963592529\n",
            "Val loss: 0.5065903790733394, Val f1: 0.7460315823554993\n",
            "Val loss: 0.5065567360204809, Val f1: 0.7463129162788391\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5551442801952362, Val f1: 0.7016494274139404\n",
            "Val loss: 0.546493798494339, Val f1: 0.7042461633682251\n",
            "Val loss: 0.5460379223028818, Val f1: 0.7013119459152222\n",
            "Val loss: 0.5485693737864494, Val f1: 0.6986367702484131\n",
            "Val loss: 0.5509171307086944, Val f1: 0.7013882398605347\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5531851500272751\n",
            "Train loss: 0.539312375314308\n",
            "Train loss: 0.5326762175559998\n",
            "Train loss: 0.5299566419266942\n",
            "Train loss: 0.5288731367105529\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4861118688302882, Val f1: 0.7732816934585571\n",
            "Val loss: 0.4895156131071203, Val f1: 0.7697796821594238\n",
            "Val loss: 0.4888846430124021, Val f1: 0.7694442272186279\n",
            "Val loss: 0.49015213987406564, Val f1: 0.7692737579345703\n",
            "Val loss: 0.491131335847518, Val f1: 0.7685024738311768\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5483174920082092, Val f1: 0.7169580459594727\n",
            "Val loss: 0.5393518954515457, Val f1: 0.7198631763458252\n",
            "Val loss: 0.538401315609614, Val f1: 0.720209002494812\n",
            "Val loss: 0.5409324988722801, Val f1: 0.7162882089614868\n",
            "Val loss: 0.5430699765682221, Val f1: 0.718865692615509\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.54512176848948\n",
            "Train loss: 0.5232167108492418\n",
            "Train loss: 0.5165979117155075\n",
            "Train loss: 0.5116130963190278\n",
            "Train loss: 0.5102618010271163\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.474348841344609, Val f1: 0.7784351110458374\n",
            "Val loss: 0.4730631156879313, Val f1: 0.7807953953742981\n",
            "Val loss: 0.4726388092134513, Val f1: 0.7803875207901001\n",
            "Val loss: 0.4725840472999741, Val f1: 0.7809457182884216\n",
            "Val loss: 0.47193964263972116, Val f1: 0.7814868092536926\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.542723149061203, Val f1: 0.7213820815086365\n",
            "Val loss: 0.5324411541223526, Val f1: 0.7259190678596497\n",
            "Val loss: 0.5310310622056326, Val f1: 0.7255124449729919\n",
            "Val loss: 0.5345318913459778, Val f1: 0.7215992212295532\n",
            "Val loss: 0.5366653800010681, Val f1: 0.7244958281517029\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5186455771327019\n",
            "Train loss: 0.5023057180823702\n",
            "Train loss: 0.4994755572080612\n",
            "Train loss: 0.49754334385715315\n",
            "Train loss: 0.4968630628926413\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.45199960996122923, Val f1: 0.8002634048461914\n",
            "Val loss: 0.45325447881923003, Val f1: 0.7996204495429993\n",
            "Val loss: 0.45498934446596634, Val f1: 0.7968974113464355\n",
            "Val loss: 0.45576740275411043, Val f1: 0.7956342101097107\n",
            "Val loss: 0.4562844651586869, Val f1: 0.7954204082489014\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5404146313667297, Val f1: 0.7219721078872681\n",
            "Val loss: 0.5291079431772232, Val f1: 0.7286244034767151\n",
            "Val loss: 0.5268566707770029, Val f1: 0.7294570803642273\n",
            "Val loss: 0.5304903239011765, Val f1: 0.7237916588783264\n",
            "Val loss: 0.532521253824234, Val f1: 0.7273485064506531\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.5033893063664436\n",
            "Train loss: 0.48752825639464636\n",
            "Train loss: 0.4846379899978638\n",
            "Train loss: 0.48110428852821463\n",
            "Train loss: 0.4811460588659559\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.43904831654885235, Val f1: 0.8014499545097351\n",
            "Val loss: 0.4378167609958088, Val f1: 0.8057281374931335\n",
            "Val loss: 0.43908729448037986, Val f1: 0.8055122494697571\n",
            "Val loss: 0.4395385941161829, Val f1: 0.8052529096603394\n",
            "Val loss: 0.43983089748550863, Val f1: 0.8050849437713623\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5381878018379211, Val f1: 0.7234162092208862\n",
            "Val loss: 0.5257538110017776, Val f1: 0.73143070936203\n",
            "Val loss: 0.5229478379090627, Val f1: 0.733193039894104\n",
            "Val loss: 0.5268774256110191, Val f1: 0.7284716963768005\n",
            "Val loss: 0.528838312625885, Val f1: 0.7316902875900269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "DHD6lLs79dce",
        "outputId": "4db0ec02-65e3-4140-eecf-64321f0b96c5"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "получается так же как с обучающимися эмбеддингами\n",
        "\n",
        "давайте попробуем изучить ошибки"
      ],
      "metadata": {
        "id": "TKnRQujN9opI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts) \n",
        "            for pred, gold, text in zip(preds, ys, texts):\n",
        "              text = ' '.join([id2word[int(symbol)] for symbol in text if symbol !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append(text)\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append(text)\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append(text)\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append(text)\n",
        "    return fp, fn, tp, tn"
      ],
      "metadata": {
        "id": "CcKW560oLPQQ"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp, fn, tp, tn = predict(model, val_iterator)"
      ],
      "metadata": {
        "id": "2e3hgD8ELek4"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('true positive: ', tp[:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YU7JscjlLvqa",
        "outputId": "71f19c7d-2eb2-49c5-ecfd-61758b705bbe"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true positive:  ['получила удовольствие от просмотра советую', 'rt ты просто на дне ещё не разу не', 'я намекаю на то что по кругу и и вообще я ты не знал', 'настолько разочаровалась в парня что задумываюсь перейти на девушек', 'спасибо накатило 100 лет эту песню не слушала', 'опиши мене за 5 слів — маленька', 'rt оказывается сегодня международный день переводчика ну что коллеги и единомышленники с праздником что ли', 'ленты только райот по к2 х мои ж', 'с вовой и проводили на задавали по 11 вопросов о коррупции поржали от души d', 'глупо задавать вопрос ответ на который итак знаешь', 'потому что я работаю на фбр и хорошо и не только', 'монголын минь хүний нэг ын бэлэг диск', 'могу набери в гугле или в хрома', 'все равно так себе смотреть надо в оригинале но это очень мило что они его показывают', 'плохие привычки нельзя уничтожить но их место можно заменить другими d', 'patrikoksi я в этом уверен меня они всегда спасают правда иногда через спасают', 'rt у мамы попрошу что бы купила мне в комнату маленькую елочку и наряжу ее по своему', 'rt ну я тогда тоже', 'festmahlmann белое что ли брать у меня как бы не дофига вариантов', 'спокойной ночи чужая ну или не чужая наверное кому как', 'скучала по тебе — воу приятно куда', 'вот это я навернулась около дома даже сама не заметила как на снегу', 'ты почти не молодец я почти благодарен тебе', 'rt gornikfanny777 встаньте дети встаньте в круг', 'justvishme моя мама видимо в одном поезде']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('ошибочно считаем позитивными: ', fp[:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMbsDskSL_Px",
        "outputId": "b4d609be-6c10-49e1-c8ea-d02e71996613"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно считаем позитивными:  ['именно ты права но тут считают что', 'маме с утра на счет этого концерта она не отпускает из-за учебы окай', 'алкоголь тут с 11 ну збс надо на ром что ли развести мать', 'уйду я от вас в злые вы все меня уже который раз все', 'тетя сказала щас снег бабушке поедем', 'уже 2-ой день подряд все красивые парни выходят на', 'да я уже с самого утра скучаю там по всем', 'это не хорошо сходи к зубному он её чем-нибудь', 'я же еще недавно спрашивала у бабушки уверенна ли она в том что вика так не и она ответила что уверенна', 'кармане 20 еще надо в автошколу потом как то домой добираться', 'день без макияжа и всее знакомые парни случайно на тебя в минске уу', 'santairis у нас вон все воду в европе им жить захотелось как будто они в африке живут мрази', 'села греться а то вообще какое-то состояние неправильное', 'потому что вот не все хлопья одинаково полезны', 'купила угги в а то ноги бы в сапогах на платформе', 'ты любишь морковку — не лезь в мою жизнь', 'и в воскресенье ты с нами не идёшь на встречу значит или что', 'у ленты хочу чтоб подарочки мне подарили', 'rt вот это я называю кто-то создал в твиттере', 'rt anuta1sth ты что все мои планы я уже билеты купила на луну думаю прилечу чай ты со своим', 'меня кубики пока неочень видно', 'испанский он такой что я схожу с ума даже без знаний испанского вы все поймете первая строчка', 'rt изза авы мне перехотелось общаться с подругой какой я', 'он думал там теперь пока автобуса', 'сны нужны для того чтобы хоть немного побыть с тем кого уже нет никогда не будет']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('ошибочно считаем негативными: ', fn[:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGtcAcq-MQWO",
        "outputId": "fd19e093-c529-47f2-e964-12c0207e25bf"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно считаем негативными:  ['кто еще сам доехал до работы кроме меня и', 'сдала две контрольные по анатомии осталась одна и свалю нафиг', 'rt я знаю мои соседи хотят послушать led zeppelin', 'хотя у меня есть верный друг который всегда со мной а значит нас двое а значит уже нихуя не страшно', 'запихнуть в вот было бы хорошо', 'а я отвечу что какой кошмар а я не пишу с видимо и не сдам х', 'все мои друзья общаются со мной в соц сетях по разному и к каждому у меня свой я не знаю какая я', 'да,я тоже хочу новогоднюю ночь с', 'rt 27_wrm mimifka ну понятно и за стулья где работу унесли и сказали здесь сиди', 'блять я к своему коту привык это же не значит что я с ним то должна быть какая-то', 'женечка спит после операции', 'сообщает о том что в москве редкие снежинки будьте осторожны за рулем снег', 'давай а то блин я же умру от скуки без тебя', 'каждую ты наверное не высыпаться я имею в', 'пиздец ночка утречко конечно зато поржали от души', 'rt завтра не будет можно будет не учить', 'сегодня увидела', 'может но ты ему все не так страшно на самом деле', 'wpnew мне хватило двух дней теперь не могу работать на старом', 'я', 'вот это последняя серия вообще крутая скорее бы второй сезон', '01znllh бы ты если не пошла учитывая что скоро собрание', 'с этой символикой сделаем', 'становится на стиль жизни', 'день не задался с самого утра а еще я поняла,что не могу без нужно рассказывать']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Честно говоря, у меня вообще большие вопросы к тому, кто размечал эти твиты... Но наверное можно объяснить часть ошибок очень просто: в твите есть какое-то слово, которое часто употребляется в позитивном ключе, например, офигенно. И если твит типа \"мама выгнала из дома, ну офигенно\", моделька прочитает слово офигенно и решит, что он радостный :)"
      ],
      "metadata": {
        "id": "sd5_TORKMrUg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## посимвольно"
      ],
      "metadata": {
        "id": "Yp-G7fDkcY3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Словарики, подготовка..."
      ],
      "metadata": {
        "id": "2wwkyGG3NAir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_s = Counter()\n",
        "for symbol in tweets_data['text']:\n",
        "    vocab_s.update(list(symbol))\n",
        "print('всего уникальных символов:', len(vocab_s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEwrQkaV9t7B",
        "outputId": "5b13b63d-6905-4268-8894-2b450d74dfc8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_vocab_s = set()\n",
        "\n",
        "for symbol in vocab_s:\n",
        "    if vocab_s[symbol] > 5:\n",
        "        filtered_vocab_s.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_vocab_s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGm3mC_hFVKv",
        "outputId": "d3509250-5308-484d-8438-d805ceaa05a8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных символов, втретившихся больше 5 раз: 211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab:\n",
        "    symbol2id[symbol] = len(symbol2id)"
      ],
      "metadata": {
        "id": "FekmgGm8FZMt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "metadata": {
        "id": "7fHcYXWyFcJI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делаем датасет, надо добавить новый словарик, и не забыть сделать поля для символов всякие."
      ],
      "metadata": {
        "id": "p_fNwKGGNDnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset2(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        tokens = self.preprocess(self.dataset[index])\n",
        "        symbols = list(self.dataset[index])\n",
        "        ids = torch.LongTensor([self.word2id[tok] for tok in tokens if tok in self.word2id])\n",
        "        ids_s = torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, ids_s, y\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        tokens = text.lower().split()\n",
        "        tokens = [token.strip(punctuation) for token in tokens]\n",
        "        tokens = [token for token in tokens if token]\n",
        "        return tokens\n",
        "        \n",
        "    def collate_fn(self, batch):\n",
        "      ids, ids_s, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      padded_ids_s = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_ids, padded_ids_s, y"
      ],
      "metadata": {
        "id": "_ZDTfsMkFfCu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "qy2gptnLIrUn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset2(train_sentences, word2id, symbol2id, device)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "metadata": {
        "id": "1AyOD-ZjGlWs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = Dataset2(train_sentences, word2id, symbol2id, device)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "metadata": {
        "id": "jrUeII0rIcxA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Моделька вроде такая как в условии задачи: эмбеддинги для слов, mean, линейный слой, эмбеддинги для символов, свертки с разным окном, макспулинг. Потом конкатенация этого всего, линейный слой и сигмоида."
      ],
      "metadata": {
        "id": "YYTBYEkXNMAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class My_model(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, s_size, embedding_dim, s_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(in_features=embedding_dim, out_features=180)\n",
        "        self.embedding_s = nn.Embedding(s_size, s_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=s_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=s_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=360, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, symb):\n",
        "        embedded = self.embedding(text).mean(axis=1)\n",
        "        embedded = self.linear1(embedded)\n",
        "        embedded_s = self.embedding_s(symb)\n",
        "        embedded_s = embedded_s.transpose(1,2)\n",
        "        feature_map_bigrams = self.bigrams(embedded_s)\n",
        "        feature_map_trigrams = self.trigrams(embedded_s)\n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        concat = torch.cat((pooling1, pooling2, embedded), 1)\n",
        "        logits = self.hidden(concat) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "metadata": {
        "id": "VLV20VhiI9NV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = My_model(len(word2id), len(symbol2id), 100, 8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "RMYOktZOOSju"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 \n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (*data, ys) in enumerate(iterator):\n",
        "        optimizer.zero_grad()  \n",
        "        preds = model(*data) \n",
        "        loss = criterion(preds, ys) \n",
        "        loss.backward()  \n",
        "        optimizer.step() \n",
        "        epoch_loss += loss.item() \n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/(i+1)}')      \n",
        "    return  epoch_loss / len(iterator) "
      ],
      "metadata": {
        "id": "2ItTFC8PPL5S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (*data, ys) in enumerate(iterator):   \n",
        "            preds = model(*data) \n",
        "            loss = criterion(preds, ys)   \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/(i+1)}, Val f1: {epoch_metric/(i+1)}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "metadata": {
        "id": "T72mkOzBP5OB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "id": "tEgG_vdIOhLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc7b8cdc-4076-4c04-b594-254fdf1875d3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py:298: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:647.)\n",
            "  self.padding, self.dilation, self.groups)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.695165052133448\n",
            "Train loss: 0.6918146259644452\n",
            "Train loss: 0.6893303464440739\n",
            "Train loss: 0.6871171821566189\n",
            "Train loss: 0.6846825648756588\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6684726476669312, Val f1: 0.6437727808952332\n",
            "Val loss: 0.670041701372932, Val f1: 0.6414141654968262\n",
            "Val loss: 0.6698340002228232, Val f1: 0.6416688561439514\n",
            "Val loss: 0.6700607029830709, Val f1: 0.6414313912391663\n",
            "Val loss: 0.6701523836921243, Val f1: 0.6403388381004333\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6696486753575942, Val f1: 0.6414955854415894\n",
            "Val loss: 0.6692235890556785, Val f1: 0.643690824508667\n",
            "Val loss: 0.6697215998873991, Val f1: 0.6416450142860413\n",
            "Val loss: 0.6698906991411658, Val f1: 0.6414837837219238\n",
            "Val loss: 0.6699846141478595, Val f1: 0.640643835067749\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6678067130201003\n",
            "Train loss: 0.6649731800836676\n",
            "Train loss: 0.6618382755447837\n",
            "Train loss: 0.6593386679887772\n",
            "Train loss: 0.6561734998927397\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.637166044291328, Val f1: 0.6556200385093689\n",
            "Val loss: 0.6356297100291532, Val f1: 0.6570250391960144\n",
            "Val loss: 0.6361021072256798, Val f1: 0.6566082835197449\n",
            "Val loss: 0.6363834177746492, Val f1: 0.655640184879303\n",
            "Val loss: 0.6357107036253985, Val f1: 0.6575056910514832\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6363821204970864, Val f1: 0.6587088108062744\n",
            "Val loss: 0.6352472007274628, Val f1: 0.659551739692688\n",
            "Val loss: 0.6355676288698234, Val f1: 0.6577229499816895\n",
            "Val loss: 0.6355441624627394, Val f1: 0.6576306819915771\n",
            "Val loss: 0.6356424107271083, Val f1: 0.657014787197113\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6344150725532981\n",
            "Train loss: 0.6320153825423297\n",
            "Train loss: 0.6291020758011762\n",
            "Train loss: 0.6253761421231663\n",
            "Train loss: 0.6216867271591635\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6012871966642492, Val f1: 0.6882929801940918\n",
            "Val loss: 0.6012576502912185, Val f1: 0.6891675591468811\n",
            "Val loss: 0.6019317858359393, Val f1: 0.6887972354888916\n",
            "Val loss: 0.6010498851537704, Val f1: 0.6904052495956421\n",
            "Val loss: 0.6014607191085816, Val f1: 0.690153956413269\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6034423884223489, Val f1: 0.6900216341018677\n",
            "Val loss: 0.6013045433689567, Val f1: 0.6928211450576782\n",
            "Val loss: 0.6014278519387338, Val f1: 0.6915701031684875\n",
            "Val loss: 0.6011614799499512, Val f1: 0.6911441087722778\n",
            "Val loss: 0.601544671198901, Val f1: 0.6904812455177307\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6022884600302753\n",
            "Train loss: 0.5989203996518079\n",
            "Train loss: 0.5946700678152197\n",
            "Train loss: 0.5926136278054294\n",
            "Train loss: 0.5902667592553531\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5715302930158728, Val f1: 0.7033151984214783\n",
            "Val loss: 0.5703941706348868, Val f1: 0.7048240303993225\n",
            "Val loss: 0.5713517525616814, Val f1: 0.7036328911781311\n",
            "Val loss: 0.570799453293576, Val f1: 0.7042139768600464\n",
            "Val loss: 0.5698151413132163, Val f1: 0.7045601606369019\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5724207443349502, Val f1: 0.7023974061012268\n",
            "Val loss: 0.5696414393537185, Val f1: 0.706574559211731\n",
            "Val loss: 0.5697790033677045, Val f1: 0.7049492001533508\n",
            "Val loss: 0.5694549127536661, Val f1: 0.7052919268608093\n",
            "Val loss: 0.5699165463447571, Val f1: 0.704321563243866\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.5694772495942957\n",
            "Train loss: 0.5672613627770368\n",
            "Train loss: 0.5644288588972652\n",
            "Train loss: 0.5635365387972664\n",
            "Train loss: 0.5613062914680033\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5420471745378831, Val f1: 0.7312899231910706\n",
            "Val loss: 0.5446992236025193, Val f1: 0.7282031178474426\n",
            "Val loss: 0.5442820775742624, Val f1: 0.7251439690589905\n",
            "Val loss: 0.544222833479152, Val f1: 0.7263354659080505\n",
            "Val loss: 0.5436002633150886, Val f1: 0.7282025218009949\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5458736805354848, Val f1: 0.7259386777877808\n",
            "Val loss: 0.5428281058283413, Val f1: 0.7289750576019287\n",
            "Val loss: 0.5430810135953567, Val f1: 0.7286285758018494\n",
            "Val loss: 0.5425885994644726, Val f1: 0.728913426399231\n",
            "Val loss: 0.5432176856433644, Val f1: 0.7281488180160522\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5392400517183191\n",
            "Train loss: 0.5393311889732585\n",
            "Train loss: 0.5374425006847755\n",
            "Train loss: 0.5369496739962522\n",
            "Train loss: 0.5363695719662834\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.524224530248081, Val f1: 0.7344986796379089\n",
            "Val loss: 0.522005228435292, Val f1: 0.7381077408790588\n",
            "Val loss: 0.5200404518959569, Val f1: 0.7400972247123718\n",
            "Val loss: 0.5199719457065358, Val f1: 0.7395675778388977\n",
            "Val loss: 0.519560810397653, Val f1: 0.7395099997520447\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5213540389257318, Val f1: 0.7379667162895203\n",
            "Val loss: 0.5183469486587188, Val f1: 0.7396100163459778\n",
            "Val loss: 0.518841371232388, Val f1: 0.7392212748527527\n",
            "Val loss: 0.5182772747734014, Val f1: 0.7398667335510254\n",
            "Val loss: 0.5189473450183868, Val f1: 0.7397472858428955\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.520317042575163\n",
            "Train loss: 0.5193303060882232\n",
            "Train loss: 0.5181353986263275\n",
            "Train loss: 0.5145240903777235\n",
            "Train loss: 0.5132742846713346\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5000225480865029, Val f1: 0.7548123002052307\n",
            "Val loss: 0.4977523693267037, Val f1: 0.7572081089019775\n",
            "Val loss: 0.49643704996389504, Val f1: 0.7588029503822327\n",
            "Val loss: 0.49691859282114925, Val f1: 0.7582283020019531\n",
            "Val loss: 0.49740172624588014, Val f1: 0.7573806643486023\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4990987917956184, Val f1: 0.7550721168518066\n",
            "Val loss: 0.4963367213221157, Val f1: 0.7563954591751099\n",
            "Val loss: 0.4969889369665408, Val f1: 0.7571893334388733\n",
            "Val loss: 0.49631019725519065, Val f1: 0.7574017643928528\n",
            "Val loss: 0.49709909312865314, Val f1: 0.7573939561843872\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.4959158231230343\n",
            "Train loss: 0.49261342427309823\n",
            "Train loss: 0.49415890258901257\n",
            "Train loss: 0.4930357008295901\n",
            "Train loss: 0.4935863032060511\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.47821827671107126, Val f1: 0.7803295850753784\n",
            "Val loss: 0.48090021487544565, Val f1: 0.7790620923042297\n",
            "Val loss: 0.48109272940486086, Val f1: 0.779558539390564\n",
            "Val loss: 0.4797069425968563, Val f1: 0.7789151072502136\n",
            "Val loss: 0.48050551274243525, Val f1: 0.7784004807472229\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4813649286242092, Val f1: 0.7777133584022522\n",
            "Val loss: 0.47900159919963164, Val f1: 0.7792999148368835\n",
            "Val loss: 0.479779699269463, Val f1: 0.7785641551017761\n",
            "Val loss: 0.47897475870216594, Val f1: 0.7784402370452881\n",
            "Val loss: 0.4799455751391018, Val f1: 0.7782164812088013\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.48428352790720325\n",
            "Train loss: 0.47881491482257843\n",
            "Train loss: 0.47485331869592856\n",
            "Train loss: 0.47460760658278184\n",
            "Train loss: 0.4756433171384475\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4679025306421168, Val f1: 0.7686991095542908\n",
            "Val loss: 0.46315808681880727, Val f1: 0.7719756960868835\n",
            "Val loss: 0.4625361012477501, Val f1: 0.7719389200210571\n",
            "Val loss: 0.4635110054822529, Val f1: 0.7707294225692749\n",
            "Val loss: 0.4639362559599035, Val f1: 0.7698823809623718\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4642949717886308, Val f1: 0.7691726088523865\n",
            "Val loss: 0.46220574133536396, Val f1: 0.7685471773147583\n",
            "Val loss: 0.4630417403052835, Val f1: 0.7690091729164124\n",
            "Val loss: 0.4624398398925276, Val f1: 0.7704225182533264\n",
            "Val loss: 0.46318458494018105, Val f1: 0.7703027129173279\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.4572942765320049\n",
            "Train loss: 0.4598691314458847\n",
            "Train loss: 0.4588684112417932\n",
            "Train loss: 0.4603072716032757\n",
            "Train loss: 0.4589374542236328\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.44983669063624215, Val f1: 0.7902449369430542\n",
            "Val loss: 0.4439199636964237, Val f1: 0.7941522598266602\n",
            "Val loss: 0.4451786232929604, Val f1: 0.7925886511802673\n",
            "Val loss: 0.44538066448534236, Val f1: 0.7925810217857361\n",
            "Val loss: 0.4457812772077673, Val f1: 0.7921567559242249\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.44662431583685036, Val f1: 0.7937353253364563\n",
            "Val loss: 0.4446020450662164, Val f1: 0.7928692698478699\n",
            "Val loss: 0.4453686382256302, Val f1: 0.7922720909118652\n",
            "Val loss: 0.4445822721018511, Val f1: 0.7927866578102112\n",
            "Val loss: 0.4455341027063482, Val f1: 0.7920732498168945\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.44573893967796774\n",
            "Train loss: 0.4463012034402174\n",
            "Train loss: 0.4455984708140878\n",
            "Train loss: 0.4446549498859574\n",
            "Train loss: 0.4448547216022716\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4314044468543109, Val f1: 0.7978594899177551\n",
            "Val loss: 0.43161146605716033, Val f1: 0.7993165850639343\n",
            "Val loss: 0.4315430323282878, Val f1: 0.8001654148101807\n",
            "Val loss: 0.43205645811908383, Val f1: 0.7998783588409424\n",
            "Val loss: 0.4316090976490694, Val f1: 0.8014076948165894\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4323897098793703, Val f1: 0.8024501204490662\n",
            "Val loss: 0.43047795839169445, Val f1: 0.8025421500205994\n",
            "Val loss: 0.43134116425233726, Val f1: 0.8018894195556641\n",
            "Val loss: 0.43053338282248554, Val f1: 0.8020908236503601\n",
            "Val loss: 0.43148412774590883, Val f1: 0.8014028072357178\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.4375656089361976\n",
            "Train loss: 0.4363707961405025\n",
            "Train loss: 0.4335273738000907\n",
            "Train loss: 0.43236488060039635\n",
            "Train loss: 0.4325404233792249\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.41630098398994, Val f1: 0.8092651963233948\n",
            "Val loss: 0.42011187882984385, Val f1: 0.8076856136322021\n",
            "Val loss: 0.4189560091963001, Val f1: 0.8098284602165222\n",
            "Val loss: 0.41965842860586505, Val f1: 0.8098913431167603\n",
            "Val loss: 0.41927463797961967, Val f1: 0.8101704716682434\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4197777947958778, Val f1: 0.8096309304237366\n",
            "Val loss: 0.41787516895462484, Val f1: 0.8105497360229492\n",
            "Val loss: 0.4186740318934123, Val f1: 0.8100293874740601\n",
            "Val loss: 0.4177928330267177, Val f1: 0.8106096982955933\n",
            "Val loss: 0.4188251958173864, Val f1: 0.8097564578056335\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.4267180807450238\n",
            "Train loss: 0.4233938622124055\n",
            "Train loss: 0.4205443695479748\n",
            "Train loss: 0.4187885965494549\n",
            "Train loss: 0.41874844502000247\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4097825551734251, Val f1: 0.8156833052635193\n",
            "Val loss: 0.4075510677169351, Val f1: 0.8177276849746704\n",
            "Val loss: 0.4082229470505434, Val f1: 0.8164668083190918\n",
            "Val loss: 0.4068699983112952, Val f1: 0.8167243599891663\n",
            "Val loss: 0.40691664569518143, Val f1: 0.8169994950294495\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.40754808397854075, Val f1: 0.8160164952278137\n",
            "Val loss: 0.4058691929368412, Val f1: 0.8170555233955383\n",
            "Val loss: 0.406647734782275, Val f1: 0.8166074752807617\n",
            "Val loss: 0.40576401484363217, Val f1: 0.8173651695251465\n",
            "Val loss: 0.4067587722750271, Val f1: 0.8165714144706726\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.40682924144408283\n",
            "Train loss: 0.40958582741372723\n",
            "Train loss: 0.40749751586540073\n",
            "Train loss: 0.4088030453113949\n",
            "Train loss: 0.40842193189789266\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3934570533387801, Val f1: 0.8213898539543152\n",
            "Val loss: 0.39716849081656513, Val f1: 0.8198876976966858\n",
            "Val loss: 0.39503448850968303, Val f1: 0.821943998336792\n",
            "Val loss: 0.3942782212706173, Val f1: 0.8224703669548035\n",
            "Val loss: 0.39515741923276115, Val f1: 0.8220506906509399\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.39602497921270485, Val f1: 0.8211150765419006\n",
            "Val loss: 0.3944326253498302, Val f1: 0.8216800093650818\n",
            "Val loss: 0.39532266177383124, Val f1: 0.8213210105895996\n",
            "Val loss: 0.39455990580951467, Val f1: 0.8221209049224854\n",
            "Val loss: 0.39552230449283826, Val f1: 0.8214142918586731\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.39485733824617725\n",
            "Train loss: 0.39574147234944734\n",
            "Train loss: 0.3964831718042785\n",
            "Train loss: 0.3968300065573524\n",
            "Train loss: 0.39623562868903667\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3924892439561732, Val f1: 0.8296718597412109\n",
            "Val loss: 0.39175348597414356, Val f1: 0.8315156102180481\n",
            "Val loss: 0.3935655045743082, Val f1: 0.8307285308837891\n",
            "Val loss: 0.39265522930551977, Val f1: 0.8305936455726624\n",
            "Val loss: 0.39165238843244665, Val f1: 0.8303797245025635\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3928043684538673, Val f1: 0.8305733799934387\n",
            "Val loss: 0.391111325691728, Val f1: 0.830888032913208\n",
            "Val loss: 0.39201271709273844, Val f1: 0.8299421668052673\n",
            "Val loss: 0.39071578006534013, Val f1: 0.8307976722717285\n",
            "Val loss: 0.3919176620595595, Val f1: 0.8301337957382202\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.3864537793047288\n",
            "Train loss: 0.38539793561486635\n",
            "Train loss: 0.38840704396659254\n",
            "Train loss: 0.3882554541615879\n",
            "Train loss: 0.3876969863386715\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.37955061653081107, Val f1: 0.8361844420433044\n",
            "Val loss: 0.37812619174227996, Val f1: 0.8375486135482788\n",
            "Val loss: 0.37729182722521765, Val f1: 0.8376786112785339\n",
            "Val loss: 0.3787204518037684, Val f1: 0.8365399837493896\n",
            "Val loss: 0.37919031206299275, Val f1: 0.8365683555603027\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3798890955307904, Val f1: 0.8361189365386963\n",
            "Val loss: 0.3782196597141378, Val f1: 0.8363446593284607\n",
            "Val loss: 0.3790973699560352, Val f1: 0.835985004901886\n",
            "Val loss: 0.3779839294798234, Val f1: 0.8366431593894958\n",
            "Val loss: 0.37913452667348524, Val f1: 0.8358960747718811\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.36914445196881013\n",
            "Train loss: 0.37371673654107485\n",
            "Train loss: 0.37707865296625626\n",
            "Train loss: 0.37947827402283163\n",
            "Train loss: 0.379727346756879\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3674685464185827, Val f1: 0.8415800333023071\n",
            "Val loss: 0.369074730312123, Val f1: 0.8409392237663269\n",
            "Val loss: 0.3704566803632998, Val f1: 0.840474009513855\n",
            "Val loss: 0.3697464505539221, Val f1: 0.8413291573524475\n",
            "Val loss: 0.3693750213174259, Val f1: 0.8412005305290222\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3698558088611154, Val f1: 0.8422211408615112\n",
            "Val loss: 0.3681175603586085, Val f1: 0.841709315776825\n",
            "Val loss: 0.36906102533433954, Val f1: 0.8409770727157593\n",
            "Val loss: 0.3679647467592183, Val f1: 0.8415411710739136\n",
            "Val loss: 0.36906954470802755, Val f1: 0.8407506942749023\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.3695934625232921\n",
            "Train loss: 0.36566894019351287\n",
            "Train loss: 0.367500444837645\n",
            "Train loss: 0.36820276563658433\n",
            "Train loss: 0.36851424715098213\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.36193007581374226, Val f1: 0.8336844444274902\n",
            "Val loss: 0.36176107911502614, Val f1: 0.8340601325035095\n",
            "Val loss: 0.36257566365541194, Val f1: 0.8341729044914246\n",
            "Val loss: 0.36048753252800775, Val f1: 0.8357093334197998\n",
            "Val loss: 0.360240598636515, Val f1: 0.8358794450759888\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.36059139931903167, Val f1: 0.8365115523338318\n",
            "Val loss: 0.35900359907571006, Val f1: 0.8356026411056519\n",
            "Val loss: 0.3598952521296108, Val f1: 0.8350032567977905\n",
            "Val loss: 0.35933979162398505, Val f1: 0.8358985781669617\n",
            "Val loss: 0.36015232836498934, Val f1: 0.8356332182884216\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.35818136790219474\n",
            "Train loss: 0.3611610777237836\n",
            "Train loss: 0.36079780085414065\n",
            "Train loss: 0.3619207757360795\n",
            "Train loss: 0.36086850306567025\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.34980934683014364, Val f1: 0.8423454165458679\n",
            "Val loss: 0.3493478859172148, Val f1: 0.8408276438713074\n",
            "Val loss: 0.3506830200260761, Val f1: 0.8408770561218262\n",
            "Val loss: 0.35125211056540995, Val f1: 0.840262770652771\n",
            "Val loss: 0.35154141853837406, Val f1: 0.8408378958702087\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.35218560169724855, Val f1: 0.8406771421432495\n",
            "Val loss: 0.35067834135364084, Val f1: 0.8410007357597351\n",
            "Val loss: 0.3516170142912397, Val f1: 0.8400816917419434\n",
            "Val loss: 0.35114232319242816, Val f1: 0.8409714102745056\n",
            "Val loss: 0.3519902965601753, Val f1: 0.8405534625053406\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.3512895282577066\n",
            "Train loss: 0.3521602171308854\n",
            "Train loss: 0.3532930799559051\n",
            "Train loss: 0.3519679605960846\n",
            "Train loss: 0.35219697882147394\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3431940850089578, Val f1: 0.8508460521697998\n",
            "Val loss: 0.34343362029861, Val f1: 0.8522936105728149\n",
            "Val loss: 0.34194080502379176, Val f1: 0.852648913860321\n",
            "Val loss: 0.34197907588061166, Val f1: 0.8515562415122986\n",
            "Val loss: 0.34213006215937, Val f1: 0.8510813117027283\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.3418945301981533, Val f1: 0.8521795272827148\n",
            "Val loss: 0.34029898310408874, Val f1: 0.8517960906028748\n",
            "Val loss: 0.3413025240103404, Val f1: 0.8508225083351135\n",
            "Val loss: 0.34056993661557927, Val f1: 0.8515554666519165\n",
            "Val loss: 0.3415154800695531, Val f1: 0.8510560989379883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aPSgldlhgQla",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "9fae7b54-20fc-4dc7-b454-abba2fcb2ddc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "хорошо работает, но мы явно обучались недостаточно. Поэтому выбираем очевидное улучшение из области гиперпараметров: увеличим learning rate. Попробуем!"
      ],
      "metadata": {
        "id": "VcLQQFI01yT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = My_model(len(word2id), len(symbol2id), 100, 8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "8fAXIZHn14Hu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4jgCoJV1_8f",
        "outputId": "1d92bd61-8328-4701-a9aa-6b9101f5a706"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 14.799325027886558\n",
            "Train loss: 7.787989225457697\n",
            "Train loss: 5.39302856314416\n",
            "Train loss: 4.179273650488433\n",
            "Train loss: 3.448862687629812\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4196363126530367, Val f1: 0.8217780590057373\n",
            "Val loss: 0.42051176902125864, Val f1: 0.8214549422264099\n",
            "Val loss: 0.41790415609584136, Val f1: 0.8233989477157593\n",
            "Val loss: 0.4189476787167437, Val f1: 0.8219126462936401\n",
            "Val loss: 0.41852806876687443, Val f1: 0.8222275972366333\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.41733359238680673, Val f1: 0.8251870274543762\n",
            "Val loss: 0.4165753739721635, Val f1: 0.8242642879486084\n",
            "Val loss: 0.4173564455088447, Val f1: 0.8230541944503784\n",
            "Val loss: 0.4168538056752261, Val f1: 0.8232453465461731\n",
            "Val loss: 0.41829064523472503, Val f1: 0.8222987651824951\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.4076569956891677\n",
            "Train loss: 0.4082500943366219\n",
            "Train loss: 0.4162032948989494\n",
            "Train loss: 0.4244437695426099\n",
            "Train loss: 0.42742509947103613\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.32765814486671896, Val f1: 0.8722346425056458\n",
            "Val loss: 0.32760660437976613, Val f1: 0.8713399171829224\n",
            "Val loss: 0.32720258539798214, Val f1: 0.8705986738204956\n",
            "Val loss: 0.32638639518443274, Val f1: 0.8712917566299438\n",
            "Val loss: 0.3268926922012778, Val f1: 0.8711031079292297\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.32691844421274524, Val f1: 0.8728488683700562\n",
            "Val loss: 0.32523353573154, Val f1: 0.8739038705825806\n",
            "Val loss: 0.3260652609899932, Val f1: 0.872803270816803\n",
            "Val loss: 0.3253241939579739, Val f1: 0.8725792169570923\n",
            "Val loss: 0.3262127655393937, Val f1: 0.8719795942306519\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.313272600664812\n",
            "Train loss: 0.3129834225949119\n",
            "Train loss: 0.32316742340723675\n",
            "Train loss: 0.3322434377144365\n",
            "Train loss: 0.34047884765793296\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.24422607702367446, Val f1: 0.906303882598877\n",
            "Val loss: 0.24557555466890335, Val f1: 0.9068949818611145\n",
            "Val loss: 0.24740982902984993, Val f1: 0.9059407711029053\n",
            "Val loss: 0.24678687313023737, Val f1: 0.906204879283905\n",
            "Val loss: 0.2479574185960433, Val f1: 0.9041428565979004\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.2505974743296118, Val f1: 0.9039778709411621\n",
            "Val loss: 0.24711134854485006, Val f1: 0.9059364199638367\n",
            "Val loss: 0.2483181187919542, Val f1: 0.9049137830734253\n",
            "Val loss: 0.2479199898593566, Val f1: 0.9046334028244019\n",
            "Val loss: 0.24816542846315048, Val f1: 0.9044153094291687\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.2363010460839552\n",
            "Train loss: 0.2298592508715742\n",
            "Train loss: 0.23537321856208876\n",
            "Train loss: 0.24823890911305652\n",
            "Train loss: 0.2588778174975339\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.1717460260671728, Val f1: 0.939397931098938\n",
            "Val loss: 0.17241736413801417, Val f1: 0.9368979334831238\n",
            "Val loss: 0.17287250418289035, Val f1: 0.9365441799163818\n",
            "Val loss: 0.17252854520783706, Val f1: 0.9368749260902405\n",
            "Val loss: 0.1721489315523821, Val f1: 0.9370977282524109\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.17184891157290516, Val f1: 0.9393414258956909\n",
            "Val loss: 0.16967161087428823, Val f1: 0.9391337633132935\n",
            "Val loss: 0.17209068788032905, Val f1: 0.9373214840888977\n",
            "Val loss: 0.1714550283025293, Val f1: 0.9379722476005554\n",
            "Val loss: 0.17217643821940704, Val f1: 0.937413215637207\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.16259823476566987\n",
            "Train loss: 0.16528308654532714\n",
            "Train loss: 0.17315979418801328\n",
            "Train loss: 0.18062768460196607\n",
            "Train loss: 0.18781680745237014\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.14444835308720083, Val f1: 0.9454503059387207\n",
            "Val loss: 0.14179562492405667, Val f1: 0.9448525905609131\n",
            "Val loss: 0.14086288856525048, Val f1: 0.9449667930603027\n",
            "Val loss: 0.14065672391477754, Val f1: 0.9452677369117737\n",
            "Val loss: 0.14073751524967307, Val f1: 0.9452559947967529\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.14126168103779063, Val f1: 0.9458133578300476\n",
            "Val loss: 0.13915258843232603, Val f1: 0.9460516571998596\n",
            "Val loss: 0.14091811679741917, Val f1: 0.9453255534172058\n",
            "Val loss: 0.14117228524649844, Val f1: 0.9453497529029846\n",
            "Val loss: 0.14132920810404945, Val f1: 0.9448646903038025\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.134860057164641\n",
            "Train loss: 0.12730638288399754\n",
            "Train loss: 0.12825641152905484\n",
            "Train loss: 0.13585707041270592\n",
            "Train loss: 0.14110499690560735\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.08461789085584528, Val f1: 0.970099151134491\n",
            "Val loss: 0.08576330104294945, Val f1: 0.9702008366584778\n",
            "Val loss: 0.0854621243243124, Val f1: 0.9705255031585693\n",
            "Val loss: 0.0860303364255849, Val f1: 0.9707465767860413\n",
            "Val loss: 0.08557926828370374, Val f1: 0.9709374904632568\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.08378501718535143, Val f1: 0.97159743309021\n",
            "Val loss: 0.0839020162820816, Val f1: 0.971807599067688\n",
            "Val loss: 0.08594121813189749, Val f1: 0.9709928631782532\n",
            "Val loss: 0.08570775214363546, Val f1: 0.9708750247955322\n",
            "Val loss: 0.08549149772700142, Val f1: 0.9710790514945984\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.08794466274626114\n",
            "Train loss: 0.0905574847231893\n",
            "Train loss: 0.0939569001396497\n",
            "Train loss: 0.09721443561070106\n",
            "Train loss: 0.10127425009713453\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.08739868266617551, Val f1: 0.9662221074104309\n",
            "Val loss: 0.08909104424802695, Val f1: 0.9660983681678772\n",
            "Val loss: 0.0884628114016617, Val f1: 0.9661806225776672\n",
            "Val loss: 0.0867192932021092, Val f1: 0.9671748876571655\n",
            "Val loss: 0.08704826389165486, Val f1: 0.9672877788543701\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.09003784595166936, Val f1: 0.9661968350410461\n",
            "Val loss: 0.08542504205423243, Val f1: 0.9679211974143982\n",
            "Val loss: 0.08732199493576498, Val f1: 0.966759204864502\n",
            "Val loss: 0.08677667861475664, Val f1: 0.9670771360397339\n",
            "Val loss: 0.08679015171878478, Val f1: 0.9672377109527588\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.07408618094290004\n",
            "Train loss: 0.07295706068329952\n",
            "Train loss: 0.07315258952040299\n",
            "Train loss: 0.07516066406798713\n",
            "Train loss: 0.08043099225443952\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.04962035662987653, Val f1: 0.983282208442688\n",
            "Val loss: 0.05018747203490313, Val f1: 0.9836469292640686\n",
            "Val loss: 0.050564954239948126, Val f1: 0.9830623269081116\n",
            "Val loss: 0.05005814469255069, Val f1: 0.9832330346107483\n",
            "Val loss: 0.048995811952387586, Val f1: 0.9835387468338013\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.04876211548552794, Val f1: 0.9836358428001404\n",
            "Val loss: 0.04840815708260326, Val f1: 0.983773946762085\n",
            "Val loss: 0.048991292751595084, Val f1: 0.9834989309310913\n",
            "Val loss: 0.049036458709879834, Val f1: 0.9835220575332642\n",
            "Val loss: 0.04904774845961262, Val f1: 0.9832062721252441\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.05404950349646456\n",
            "Train loss: 0.056729241876917726\n",
            "Train loss: 0.058482577330341526\n",
            "Train loss: 0.06444043156636112\n",
            "Train loss: 0.06662504975410069\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.04096133623491315, Val f1: 0.9845407009124756\n",
            "Val loss: 0.041772136026445555, Val f1: 0.9847972989082336\n",
            "Val loss: 0.04177030300100645, Val f1: 0.985123336315155\n",
            "Val loss: 0.04146288740722572, Val f1: 0.985357403755188\n",
            "Val loss: 0.04136557723669445, Val f1: 0.9852257370948792\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.04358731034924002, Val f1: 0.9837921261787415\n",
            "Val loss: 0.041137449662475026, Val f1: 0.9855319857597351\n",
            "Val loss: 0.04194243719764784, Val f1: 0.9853707551956177\n",
            "Val loss: 0.041975955780157274, Val f1: 0.9852426052093506\n",
            "Val loss: 0.04139046669006348, Val f1: 0.9854579567909241\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.045969942475066465\n",
            "Train loss: 0.047006531235049755\n",
            "Train loss: 0.05013862566328516\n",
            "Train loss: 0.05366032751386657\n",
            "Train loss: 0.05565312513533761\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.03869167463306118, Val f1: 0.9868103265762329\n",
            "Val loss: 0.03602967542760512, Val f1: 0.9872384667396545\n",
            "Val loss: 0.036038200937065425, Val f1: 0.9874075651168823\n",
            "Val loss: 0.0364252514041522, Val f1: 0.9870398044586182\n",
            "Val loss: 0.03634875353206606, Val f1: 0.9871529340744019\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.03742358636330156, Val f1: 0.9874849915504456\n",
            "Val loss: 0.035754142876933605, Val f1: 0.9879152774810791\n",
            "Val loss: 0.0364499472300796, Val f1: 0.987382709980011\n",
            "Val loss: 0.03643947889042251, Val f1: 0.9872028231620789\n",
            "Val loss: 0.03661919556119863, Val f1: 0.9870167374610901\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.04303580434883342\n",
            "Train loss: 0.045088947038440144\n",
            "Train loss: 0.048810935560979096\n",
            "Train loss: 0.05309134069830179\n",
            "Train loss: 0.05723938854301677\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.04105735515408656, Val f1: 0.9858691096305847\n",
            "Val loss: 0.04302941826993928, Val f1: 0.984865665435791\n",
            "Val loss: 0.04214485891747708, Val f1: 0.9848330020904541\n",
            "Val loss: 0.041404784465318215, Val f1: 0.984935998916626\n",
            "Val loss: 0.04110750766361461, Val f1: 0.9849352836608887\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.03898600675165653, Val f1: 0.9857712984085083\n",
            "Val loss: 0.03982665888307726, Val f1: 0.9854456782341003\n",
            "Val loss: 0.04056702853710044, Val f1: 0.985287070274353\n",
            "Val loss: 0.0413380194838871, Val f1: 0.9850555062294006\n",
            "Val loss: 0.04150194633094703, Val f1: 0.9849201440811157\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.044893860159551394\n",
            "Train loss: 0.047631749335457295\n",
            "Train loss: 0.05030801286007844\n",
            "Train loss: 0.055177734014304244\n",
            "Train loss: 0.057404056629713845\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.039714774883845276, Val f1: 0.985525906085968\n",
            "Val loss: 0.036786144873236906, Val f1: 0.9866563081741333\n",
            "Val loss: 0.03737802409073886, Val f1: 0.9866631627082825\n",
            "Val loss: 0.037053455735611564, Val f1: 0.9867886900901794\n",
            "Val loss: 0.03668347739559762, Val f1: 0.9869999885559082\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.03676618942443062, Val f1: 0.986819326877594\n",
            "Val loss: 0.03543595903936554, Val f1: 0.9874062538146973\n",
            "Val loss: 0.036889391458209825, Val f1: 0.9867411255836487\n",
            "Val loss: 0.03689155042828882, Val f1: 0.9867463111877441\n",
            "Val loss: 0.03667384819949374, Val f1: 0.9869222640991211\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.03749537270735292\n",
            "Train loss: 0.04164846909835058\n",
            "Train loss: 0.04403328537648799\n",
            "Train loss: 0.06156120175386176\n",
            "Train loss: 0.1094755058779436\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.1682434117092806, Val f1: 0.9549781680107117\n",
            "Val loss: 0.1671557991820223, Val f1: 0.954437792301178\n",
            "Val loss: 0.16272567037273855, Val f1: 0.955489993095398\n",
            "Val loss: 0.16361215688726483, Val f1: 0.9549503326416016\n",
            "Val loss: 0.16269415634519913, Val f1: 0.9548321962356567\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.15493777569602518, Val f1: 0.9565085172653198\n",
            "Val loss: 0.16005191995817072, Val f1: 0.9555478692054749\n",
            "Val loss: 0.16239781093363667, Val f1: 0.9547083377838135\n",
            "Val loss: 0.16227596932474306, Val f1: 0.9548293948173523\n",
            "Val loss: 0.16368827960070442, Val f1: 0.9546467065811157\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.1391953961814151\n",
            "Train loss: 0.15256081039414687\n",
            "Train loss: 0.1429582596117375\n",
            "Train loss: 0.13878423391896136\n",
            "Train loss: 0.13465647855225732\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.04984070273006663, Val f1: 0.9815338253974915\n",
            "Val loss: 0.049222186591257065, Val f1: 0.9814634919166565\n",
            "Val loss: 0.05150722175398294, Val f1: 0.98082435131073\n",
            "Val loss: 0.05209731494131334, Val f1: 0.9805904626846313\n",
            "Val loss: 0.05200238751576227, Val f1: 0.9805526733398438\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.05277934210265384, Val f1: 0.979952335357666\n",
            "Val loss: 0.04989788111518411, Val f1: 0.9815947413444519\n",
            "Val loss: 0.05169397125057146, Val f1: 0.9809616208076477\n",
            "Val loss: 0.05207015513716375, Val f1: 0.9806550145149231\n",
            "Val loss: 0.05237161854610724, Val f1: 0.9807135462760925\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.0575010230874314\n",
            "Train loss: 0.057675943764693594\n",
            "Train loss: 0.055440957128417255\n",
            "Train loss: 0.05484546781243647\n",
            "Train loss: 0.05559180812800632\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.02758835442364216, Val f1: 0.9895451664924622\n",
            "Val loss: 0.026560837457723478, Val f1: 0.9901332855224609\n",
            "Val loss: 0.025981465244994444, Val f1: 0.9903034567832947\n",
            "Val loss: 0.025380199364222148, Val f1: 0.9909102320671082\n",
            "Val loss: 0.025503204543800916, Val f1: 0.9908795952796936\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.025628264226457653, Val f1: 0.9913580417633057\n",
            "Val loss: 0.024686363224378404, Val f1: 0.9912253022193909\n",
            "Val loss: 0.025501249668498833, Val f1: 0.9910649657249451\n",
            "Val loss: 0.02569033219205106, Val f1: 0.9908429384231567\n",
            "Val loss: 0.025481529884478625, Val f1: 0.9909571409225464\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.02975026156534167\n",
            "Train loss: 0.02674299541532117\n",
            "Train loss: 0.02756887632330843\n",
            "Train loss: 0.02935046673861935\n",
            "Train loss: 0.030356191131560243\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.019153541282695884, Val f1: 0.9941743016242981\n",
            "Val loss: 0.019463023992584032, Val f1: 0.9938421845436096\n",
            "Val loss: 0.019387548241545174, Val f1: 0.9937990307807922\n",
            "Val loss: 0.018442415139254403, Val f1: 0.9939807653427124\n",
            "Val loss: 0.018147707171738147, Val f1: 0.9940117597579956\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.01777707741541021, Val f1: 0.9951737523078918\n",
            "Val loss: 0.01747705987380708, Val f1: 0.9948533177375793\n",
            "Val loss: 0.01818905277725528, Val f1: 0.9943378567695618\n",
            "Val loss: 0.018314214395906997, Val f1: 0.9940462708473206\n",
            "Val loss: 0.018230955138364258, Val f1: 0.9940966367721558\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.01814894624711836\n",
            "Train loss: 0.018927456482368356\n",
            "Train loss: 0.019990083524117283\n",
            "Train loss: 0.02067103869665195\n",
            "Train loss: 0.021356092919321622\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.012958150220048778, Val f1: 0.9952922463417053\n",
            "Val loss: 0.01479655849363874, Val f1: 0.9947952032089233\n",
            "Val loss: 0.01439868578431653, Val f1: 0.9948267936706543\n",
            "Val loss: 0.014066932832493502, Val f1: 0.9950991272926331\n",
            "Val loss: 0.013973396654952975, Val f1: 0.995243489742279\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.013411368298179963, Val f1: 0.9952767491340637\n",
            "Val loss: 0.013308664783835411, Val f1: 0.9953598976135254\n",
            "Val loss: 0.014105151754383947, Val f1: 0.9952182173728943\n",
            "Val loss: 0.014363096342148149, Val f1: 0.9950323700904846\n",
            "Val loss: 0.014058810406747986, Val f1: 0.9951391220092773\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.01783513748908744\n",
            "Train loss: 0.018170645131784326\n",
            "Train loss: 0.01757555924282939\n",
            "Train loss: 0.018383238231763244\n",
            "Train loss: 0.019202051235034184\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.012904715669505736, Val f1: 0.9957294464111328\n",
            "Val loss: 0.012482548242106158, Val f1: 0.9958052039146423\n",
            "Val loss: 0.012805458203907692, Val f1: 0.9956294894218445\n",
            "Val loss: 0.01283563463263871, Val f1: 0.9955891966819763\n",
            "Val loss: 0.01249983198940754, Val f1: 0.9957037568092346\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.01192821200717898, Val f1: 0.9954854249954224\n",
            "Val loss: 0.011863125310115078, Val f1: 0.9956938624382019\n",
            "Val loss: 0.012266746831729132, Val f1: 0.9956566095352173\n",
            "Val loss: 0.01228677826550077, Val f1: 0.9957167506217957\n",
            "Val loss: 0.012558150773539263, Val f1: 0.9956398606300354\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.014335164060706602\n",
            "Train loss: 0.014967952101655742\n",
            "Train loss: 0.01605737489629902\n",
            "Train loss: 0.017027709503000713\n",
            "Train loss: 0.017998067437506773\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.01188018572900225, Val f1: 0.9955015778541565\n",
            "Val loss: 0.015622890390017453, Val f1: 0.9951497316360474\n",
            "Val loss: 0.015086168277205205, Val f1: 0.99495530128479\n",
            "Val loss: 0.014318030382342199, Val f1: 0.9951916933059692\n",
            "Val loss: 0.01386603714679094, Val f1: 0.9953159093856812\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.012535980628693806, Val f1: 0.9950507879257202\n",
            "Val loss: 0.01259428251753835, Val f1: 0.9953433871269226\n",
            "Val loss: 0.012839976762074466, Val f1: 0.9952906966209412\n",
            "Val loss: 0.012776048949864857, Val f1: 0.9953323006629944\n",
            "Val loss: 0.01389939682010342, Val f1: 0.9952968955039978\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.01566746589892051\n",
            "Train loss: 0.015201224041554858\n",
            "Train loss: 0.015175128191271248\n",
            "Train loss: 0.015865552204880205\n",
            "Train loss: 0.01745223141658832\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.01814738367958104, Val f1: 0.9955717921257019\n",
            "Val loss: 0.014106325216262657, Val f1: 0.9960274696350098\n",
            "Val loss: 0.01343980348468119, Val f1: 0.9958450198173523\n",
            "Val loss: 0.012720867190236115, Val f1: 0.9958933591842651\n",
            "Val loss: 0.012220858546960003, Val f1: 0.9958662986755371\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.011843898799270391, Val f1: 0.9953506588935852\n",
            "Val loss: 0.011058997342308216, Val f1: 0.9960505366325378\n",
            "Val loss: 0.01117640915874611, Val f1: 0.9959221482276917\n",
            "Val loss: 0.011269083359580049, Val f1: 0.9958896636962891\n",
            "Val loss: 0.012175303015529233, Val f1: 0.9959126114845276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "izI5cm6Q4q8R",
        "outputId": "342106d0-9664-4ee5-a416-31e50dae3bd6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "это работает потрясающе."
      ],
      "metadata": {
        "id": "LG2xDOlw0g_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем поанализировать\n",
        "\n"
      ],
      "metadata": {
        "id": "S7cjxbx07X6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, symbs, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, symbs) \n",
        "            for pred, gold, text, symb in zip(preds, ys, texts, symbs):\n",
        "              text = ' '.join([id2word[int(symbol)] for symbol in text if symbol !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append(text)\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append(text)\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append(text)\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append(text)\n",
        "    return fp, fn, tp, tn"
      ],
      "metadata": {
        "id": "SxV1QyqG9WNm"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp, fn, tp, tn = predict(model, val_iterator)"
      ],
      "metadata": {
        "id": "wQg_4_Op7twT"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('true positive: ', tp[:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4c4-Pk4Fr5n",
        "outputId": "c08d1f69-94ab-449a-c035-db433a9a9c42"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true positive:  ['de_nativo будем считать что красные яблочки и что-то там ещё уважения цветам деда мороза', 'в том что это может быть я не в одиночестве', 'sassy_lolz да ведь не за что это просто мнение мое', 'только похоже я одна его буду пить потому что лера за зож а у оли парень и печень', 'ахаха саша блин как всегда просто иди нахуй это вечное', 'sport_by если команда так им выходные без даши', 'ну ты-то меня понимаешь знаю я как ты любишь жрать', 'даже и не знаю,что лучше жить с тобой или жить дома можно ведь дома с тобой d', 'планы на неделю в форму курсач делать новый кубок с личной жизнью сдохнуть', 'usastupidd воть про майдан', 'и для эффекта сразу серию после 6-й', 'и кстати в борьбе за русь', 'ахах сейчас меня моя физручка тогда будет должна вам', 'русская версия про выглядит всё-таки', 'с утра завтра сделаю математику схожу в ванную и в школоло на третий урок', 'meetprettyshit классно с я буду очень ждать это письмецо', 'от скуки превращаюсь в твит в секунду но твой рекорд никому не побить', 'а сейчас я очень устала так что пора идти спааать', 'так здорово вика с питера уже получила мой новогодний подарок делать приятное людям особенно очень круто', 'мне шоколадку ради тебя выложила я ем ее она нереально вкусная', 'оооо мне еще столько надо просмотреть', 'євромайдан историю было бы было бы украины', 'в топе на жуйце ненависть к онеме вах мир меняецо', 'rt друзья летит из во владивосток в ближайшее время за ретвит', 'в у директора забрал почистил пиздец всему теперь меня не ищите']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('ошибочно не относим к позитивным:', fn[:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKEq7XyL8XX5",
        "outputId": "bec2b515-39e9-4857-e71f-33d295aae930"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно не относим к позитивным: ['ты только что пришла в школу сколько можно', 'ахахаха я долго не мог потом понял дело в', 'а вообще я кажется купила все для нового года и это такое облегчение', 'а что с', 'люблю спать', 'ааа это реально ужасный момент был я то думала ты 2 часть смотришь', 'у нас тут тоже новогоднее настроение', 'ты его не могу ж его', 'и не', 'ахаха я свой ответ и поняла что тоже ничего не', 'я ну', 'а вот и нет один раз и все', 'я не а', 'но всегда и', 'а я вообще на тебя', 'зато они там совсем дождаться фото', 'в школе надо по поем', 'вряд ли меня когда-нибудь лучше', 'ну это а у меня', 'мне уже тоже', 'это было так не похоже на него teamfollowback', 'rt не', 'rt хочу нг в школе много как это все', 'я тоже', 'завтра еду на по немного а пока всем во']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('ошибочно считаем позитивными:', fp[:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYVlSbtL8ihn",
        "outputId": "45807a26-dd0f-41ab-da9c-e460511ab7df"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно считаем позитивными: ['rt ну', 'пока ты ее не получишь я не', '', 'спасибо', 'короче второй урок я уже пойду на третий не хочу идти вообще', 'а мне есть куда но папа сказал что даже не подумает мне его купить', 'rt же', '', 'доброе утро где ты', 'не хочу тебя', '', 'rt ты', 'один раз и я почти с нее', 'с ним', 'утро начинается не с кофе', 'rt со', 'ахаха ты бы видела кто мне это', '', 'rt', 'rt', 'а может вас как', '', 'и', 'желание на новый год', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Многие из этих твитов короткие, или непонятные, или как будто не целые. В целом, я бы сама испытывала трудности их оценить))"
      ],
      "metadata": {
        "id": "4WPCosYNF4eu"
      }
    }
  ]
}